{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT that can generate molecules from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Molecules can be represented in multitude of ways. \n",
    "One of the most widely used representations is to use text, for example in the so-called SMILES notation.\n",
    "In SMILES notation, a molecule is represented as a string of characters, where each character represents an atom or a bond.\n",
    "For example, the SMILES notation for ethanol is `CCO`. The one for benzene is `c1ccccc1`. You see that hydrogen atoms are typically omitted in SMILES notation, and that lower case letters are used for aromatic atoms. \n",
    "There is a [full grammar for SMILES notation](http://opensmiles.org/opensmiles.html) and [various alternative representations](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf), but we will stick to this simple version for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Important problems that our final solution will need to be able to solve are: \n",
    "\n",
    "- dealing with inputs of different lengths (e.g, different number of atoms in different molecules)\n",
    "- incorporating information about the semantic meaning of the atoms in the molecule (to obtain meaningful molecules, the model, e.g., should probably \"know\" what kind of bonds carbon can form)\n",
    "- dealing with the interaction between atoms in the molecule (not all arrangements of atoms are equally likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The one with different result ! \n",
    "import pandas as pd \n",
    "from rdkit import Chem\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import re \n",
    "from typing import List\n",
    "import numpy as np \n",
    "from math import exp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are helper functions that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model):\n",
    "    \"\"\"Return the number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_num_parameters_per_layer(model):\n",
    "    \"\"\"Return the number of trainable parameters in the model per layer.\"\"\"\n",
    "    layers = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            layers[name] = p.numel()\n",
    "    return layers\n",
    "\n",
    "\n",
    "def set_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        if torch.backends.mps.is_built():\n",
    "            device = 'mps'\n",
    "    elif torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Dealing with SMILES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Before we can do anything, we need to obtain data.  For doing so, we will need a dataset of SMILES strings. We will use the [ZINC dataset](https://zinc.docking.org/) which is a public database of commercially-available compounds. We will use the `250k` subset of the dataset which contains 250,000 compounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-16 09:35:47--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz\n",
      "Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 52.219.120.121, 3.5.163.14, 52.219.113.33, ...\n",
      "Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|52.219.120.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6941580 (6.6M) [application/x-gzip]\n",
      "Saving to: ‘zinc15_250K_2D.tar.gz’\n",
      "\n",
      "zinc15_250K_2D.tar. 100%[===================>]   6.62M  4.62MB/s    in 1.4s    \n",
      "\n",
      "2024-08-16 09:35:50 (4.62 MB/s) - ‘zinc15_250K_2D.tar.gz’ saved [6941580/6941580]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz'\n",
    "!tar -xzf zinc15_250K_2D.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading and extracting the dataset, we can load it into memory and take a look at some molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('zinc15_250K_2D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f30bec72110>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmiles(\"C=COCCc1Cc1F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue any further, we will also create train/valid and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torch.utils.data.random_split(df['smiles'], [200000, 25000, 25000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a language model, we will need to split the SMILES into tokens. Tokens are the smallest units of text that the model will work with.\n",
    "The model will learn to predict a molecule token by token.\n",
    "There is not one correct way to do this, but one very common way is to split the SMILES into \"chemical tokens\". For this, [Philippe Schwaller wrote down a regular expression](https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e).\n",
    "\n",
    "Commonly used other tokenization methods are:\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece)\n",
    "- [Byte-Pair Encoding (BPE)](https://github.com/openai/tiktoken)\n",
    "\n",
    "::: {.callout-note}\n",
    "Some try to move completely away from tokenization and [directly](https://byte-gpt.github.io/) [model](https://www.youtube.com/watch?v=kcd0BTKJuXk) [bytes](https://arxiv.org/abs/2105.13626).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(smiles: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES string\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of tokens\n",
    "    \"\"\"\n",
    "    SMI_REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "    \n",
    "    return re.findall(SMI_REGEX_PATTERN, smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The molecule, CCO (ethanol), is tokenized as ['C', 'C', 'O']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'C', 'O']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('CCO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting tokens into IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inputing tokens into a model, we will need to convert them into numbers.\n",
    "\n",
    "To do so, we will set up a \"vocabulary\" which is a dictionary that maps tokens to integers.\n",
    "The vocabulary also defines the tokens that are known to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special tokens\n",
    "\n",
    "Our model will be fed sequences of fixed length. Our SMILES, however, are of variable length. \n",
    "We will have to pad them to a fixed length. We will use a padding token for this purpose. \n",
    "That is, we will add a specific \"[PAD]\" token to the vocabulary which only serves the purpose of padding.\n",
    "\n",
    "Often, we also add other tokens such as `[EOS]` (end of sequence) or `[BOS]` (beginning of sequence).\n",
    "\n",
    "They are typically used as follows:\n",
    "\n",
    "- `[BOS]` is added at the beginning of each sequence\n",
    "- `[EOS]` is added at the end of each sequence\n",
    "- `[PAD]` is added to the end of each sequence to pad it to a fixed length\n",
    "- `[UNK]` is used to replace tokens that are not in the vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all of this together in a `Tokenizer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokens: List[str], eos: str = '[EOS]', bos: str = '[BOS]', pad: str = '[PAD]', unk: str = '[UNK]'):\n",
    "        self.tokens = [pad, bos, eos, unk] + tokens\n",
    "        self._token_to_index = {token: index for index, token in enumerate(self.tokens)}\n",
    "        self.index_to_token = {index: token for index, token in enumerate(self.tokens)}\n",
    "\n",
    "    def token_to_index(self, token: str) -> int:\n",
    "        try:\n",
    "            return self._token_to_index[token]\n",
    "        except KeyError:\n",
    "            return self._token_to_index['[UNK]']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.token_to_index[item]\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.tokens\n",
    "\n",
    "    def encode(self, smiles: str, add_sos: bool=False, add_eos: bool=False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode a SMILES into a list of indices\n",
    "\n",
    "        Args:\n",
    "            smiles (str): SMILES string\n",
    "            add_sos (bool): Add start of sentence token\n",
    "            add_eos (bool): Add end of sentence token\n",
    "\n",
    "        Returns:\n",
    "            List[int]: List of indices\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        if add_sos:\n",
    "            tokens.append(self.token_to_index('[BOS]'))\n",
    "        tokens += [self.token_to_index(token) for token in tokenize(smiles)]\n",
    "        if add_eos:\n",
    "            tokens.append(self.token_to_index('[EOS]'))\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, indices: List[int], strip_special_tokens: bool = True) -> str: \n",
    "        \"\"\"\n",
    "        Decode a list of indices into a SMILES\n",
    "\n",
    "        Args:\n",
    "            indices (List[int]): List of indices\n",
    "        \n",
    "        Returns:\n",
    "            str: SMILES string\n",
    "        \"\"\"\n",
    "        decoded = ''.join([self.index_to_token[index] for index in indices])\n",
    "        if strip_special_tokens:\n",
    "            return decoded.replace('[PAD]', '').replace('[BOS]', '').replace('[EOS]', '')\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate the tokenizer, we need to pass the list of tokens that we want to use. (This is sometimes called \"training\" the tokenizer, but in this case, we are just defining the tokens that we want to use.)\n",
    "We will use the following tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "lengths = []\n",
    "for smiles in train.dataset.values:\n",
    "    tokens_ = tokenize(smiles)\n",
    "    tokens.update(tokens_)\n",
    "    lengths.append(len(tokens_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.0000e+00, 4.0000e+00, 7.0000e+00, 0.0000e+00, 2.3000e+01,\n",
       "        5.6000e+01, 0.0000e+00, 7.8000e+01, 2.0100e+02, 0.0000e+00,\n",
       "        3.8900e+02, 8.0200e+02, 1.4320e+03, 0.0000e+00, 2.5760e+03,\n",
       "        3.9450e+03, 0.0000e+00, 5.8570e+03, 8.0820e+03, 0.0000e+00,\n",
       "        1.0313e+04, 1.2675e+04, 1.4914e+04, 0.0000e+00, 1.7137e+04,\n",
       "        1.8718e+04, 0.0000e+00, 2.0510e+04, 2.0796e+04, 0.0000e+00,\n",
       "        2.1073e+04, 2.0330e+04, 1.8396e+04, 0.0000e+00, 1.6193e+04,\n",
       "        1.2172e+04, 0.0000e+00, 9.8210e+03, 5.8470e+03, 0.0000e+00,\n",
       "        3.9460e+03, 2.1220e+03, 9.6800e+02, 0.0000e+00, 4.1200e+02,\n",
       "        1.4500e+02, 0.0000e+00, 4.6000e+01, 1.0000e+01, 1.0000e+00]),\n",
       " array([17. , 17.7, 18.4, 19.1, 19.8, 20.5, 21.2, 21.9, 22.6, 23.3, 24. ,\n",
       "        24.7, 25.4, 26.1, 26.8, 27.5, 28.2, 28.9, 29.6, 30.3, 31. , 31.7,\n",
       "        32.4, 33.1, 33.8, 34.5, 35.2, 35.9, 36.6, 37.3, 38. , 38.7, 39.4,\n",
       "        40.1, 40.8, 41.5, 42.2, 42.9, 43.6, 44.3, 45. , 45.7, 46.4, 47.1,\n",
       "        47.8, 48.5, 49.2, 49.9, 50.6, 51.3, 52. ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lengths, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('CCO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we only encode the SMILES strings into a list of indices. \n",
    "There is no inherent meaning to the indices themselves, and we can improve modeling by representing each index as a vector. \n",
    "We call those vectors embeddings, but they are nothing more than a vector representation--like a feature vector--for each index.\n",
    "\n",
    "Ideally, those vectors ensure that similar indices are close to each other in the embedding space.\n",
    "There are many ways to create those embeddings. But for now it is only important to know this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings we just created contain only information about their identity. \n",
    "However, they contain no information about their position in the sequence.\n",
    "\n",
    "To add positional information, we can add a positional encoding to the embeddings. Again, there are many ways to do this.\n",
    "\n",
    "A very simple way is called _absolute positional encoding_. For this we simply add the position index to the embedding vector.\n",
    "\n",
    "For example \n",
    "\n",
    "```python\n",
    "B, T, C = 2, 3, 4 # batch size, sequence length, embedding size\n",
    "x = torch.rand(B, T, C)\n",
    "pos = torch.arange(T).unsqueeze(0).repeat(B, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language modeling dataset\n",
    "\n",
    "A dataset class is a class that inherits from `torch.utils.data.Dataset`. It is used to load data into a model.\n",
    "\n",
    "The most important methods of a dataset class are:\n",
    "\n",
    "- `__len__`: This method returns the length of the dataset. It is used by the `DataLoader` to determine how many batches to load.\n",
    "- `__getitem__`: This method returns a single sample from the dataset. It is used by the `DataLoader` to load a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLanguageModelingDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for text in texts:\n",
    "            input_ids = np.array(tokenizer.encode(text))\n",
    "            if len(input_ids) > self.max_length:\n",
    "                continue\n",
    "            input_ids = self._pad_right(input_ids, self.max_length)\n",
    "            # make next token the target create datasets with sliding windows\n",
    "            for i in range(1, len(input_ids)):\n",
    "                self.inputs.append(self._pad_left(input_ids[:i], self.max_length))\n",
    "                self.targets.append([input_ids[i]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]\n",
    "        target_ids = self.targets[idx]\n",
    "\n",
    "        return  torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long)\n",
    "    \n",
    "    def _pad_left(self, sequence, max_len):\n",
    "        pad_value = self.tokenizer.token_to_index('[PAD]')\n",
    "        padded_sequence = np.full(max_len, pad_value)\n",
    "        padded_sequence[-len(sequence):] = sequence\n",
    "        return padded_sequence\n",
    "\n",
    "    def _pad_right(self, sequence, max_len):\n",
    "        pad_value = self.tokenizer.token_to_index('[PAD]')\n",
    "        padded_sequence = np.full(max_len, pad_value)\n",
    "        padded_sequence[:len(sequence)] = sequence\n",
    "        return padded_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You hopefully note something very interesting in this dataset: Based on one SMILES, we can create multiple training examples, because we can slide a window over the SMILES and predict the next token.  \n",
    "\n",
    "\n",
    "**Note that our implementation is relatively naiive and is optimized to make this point clear. In practice, you should use dedicated methods, e.g., from the `transformers` library, to create language model datasets. I.e. instead of predicting only one token, we will directly predict the next tokens for all tokens in the input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest language model is a bigram model. In a bigram model, we predict the next token based on the previous token.\n",
    "A bigram model is the simplest form of `n-gram` model. In an `n-gram` model, we predict the next token based on the previous `n` tokens.\n",
    "\n",
    "$N$-gram models are a simple but effective way to model language. The idea is to predict the next word in a sentence given the previous $n-1$ words. For example, in a 2-gram (bigram) model, we would predict the next word given only the previous word. In a 3-gram model, we would predict the next word given the previous two words. In general, we would predict the next word given the previous $n-1$ words.\n",
    "\n",
    "Formally, we can write down the bigram model as follows:\n",
    "\n",
    "$$\n",
    "p(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\n",
    "$$\n",
    "\n",
    "where $w_i$ is the $i$-th word in the sentence, $C(w_{i-1}, w_i)$ is the number of times the bigram $w_{i-1}, w_i$ occurs in the training set, and $C(w_{i-1})$ is the number of times the word $w_{i-1}$ occurs in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the bigram model only considers the previous word/token, we only need a lookup table. \n",
    "\n",
    "Such lookup tables are implemented in PyTorch as `nn.Embedding`. Keep in mind that an embedding layer is nothing fancy. It works like inputting a one-hot encoded vector in a linear layer:\n",
    "\n",
    "\n",
    "::: {.callout-note title=\"What are embedding layers?\"}\n",
    "Sebastian Raschka made a great figure about that.\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-media-max-width=\"560\"><p lang=\"en\" dir=\"ltr\">Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.<br>But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups. <a href=\"https://t.co/0I3AFk4por\">pic.twitter.com/0I3AFk4por</a></p>&mdash; Sebastian Raschka (@rasbt) <a href=\"https://twitter.com/rasbt/status/1611401567030083587?ref_src=twsrc%5Etfw\">January 6, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "You can try it yourself using the following code (taken from Sebastian's tweet):\n",
    "\n",
    "You can first use an embedding layer to encode the indices and then use a linear layer to do the same. You will see that the results are the same.\n",
    "\n",
    "Here for example, we encode the indices `[2, 3, 1]` into a 5-dimensional vector using an embedding layer and a linear layer.\n",
    "```python\n",
    "torch.manual_seed(123);\n",
    "idx = torch.tensor([2, 3, 1]) # 3 training examples\n",
    "num_idx = max(idx)+1\n",
    "out_dim = 5\n",
    "embedding = torch.nn.Embedding(num_idx, out_dim)\n",
    "embedding(idx)\n",
    "```\n",
    "\n",
    "The code for the linear layer is:\n",
    "```python\n",
    "\n",
    "torch.manual_seed(123);\n",
    "idx = torch.tensor([2, 3, 1]) # 3 training examples\n",
    "one_hot = torch.nn.functional.one_hot(idx, num_classes=num_idx)\n",
    "linear = torch.nn.Linear(num_idx, out_dim, bias=False)\n",
    "linear.weight = torch.nn.Parameter(embedding.weight.T.detach()) # nn.Linear does xW^T, so we need to transpose the weight matrix\n",
    "linear(one_hot.float())\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Embedding` layer, we can create a simple Bigram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 40):\n",
    "        super().__init__()\n",
    "        # \"learnable dictionary\" that maps one token to another token\n",
    "        self.mapping_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # the forward pass only consists of a lookup in the mapping layer\n",
    "        return self.mapping_layer(x)\n",
    "    \n",
    "    def loss(self, x, y): \n",
    "        # x has shape (batch_size, sequence_length)\n",
    "        predictions = self.forward(x)\n",
    "        B, T, C = predictions.shape\n",
    "        \n",
    "        # predictions has shape (batch_size, sequence_length, vocab_size)\n",
    "        predictions = predictions.view(B*T, C)\n",
    "        \n",
    "        # y has the shape (batch_size, sequence_length)\n",
    "        y = y.view(B*T)\n",
    "\n",
    "        # we use cross entropy loss to train the model\n",
    "        return F.cross_entropy(predictions, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramModel(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a token ID, the model predict how likely each token of the vocabulary is to be the next. Right now, the model is not trained, so it will predict the next token randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125/730608109.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(bigram(torch.tensor([1])))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1563, 0.1344, 0.0131, 0.1118, 0.2667, 0.1728, 0.0150, 0.0396, 0.0405,\n",
       "         0.0500]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(bigram(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating a sequence, we can implement a `generate` method that iteratively predicts the next token and appends it to the sequence. We can then use this method to generate a sequence of a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read of the logits of the next token from table\n",
    "        self.mapping_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        return self.mapping_table(x) # returns tensor of shape (batch_size, time_steps, vocab_size)\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        logits = self.forward(x) # (B, T, C)\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Note that that the implementation below is because of how we - for educational purposes - have defined the dataset\n",
    "        # A better way is to have inputs and outputs of the same length (and to not manually code the sliding window\n",
    "        # but to instead use a causal mask)\n",
    "\n",
    "        # in our case, y only contains the next token\n",
    "        # so we only care about the last token in Bigram\n",
    "        logits = logits[:, -1, :]\n",
    "        logits = logits.view(B, C)\n",
    "        y = y.view(B)\n",
    "        \n",
    "        return F.cross_entropy(logits, y)\n",
    "    \n",
    "    def generate(self, x, max_new_tokens=100):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        # we generate max_new_tokens new tokens\n",
    "        new_tokens = []\n",
    "        for _t in range(max_new_tokens):\n",
    "            logits = self.forward(x) # (B, T, C)\n",
    "\n",
    "            logits = logits[:, -1, :] # we only care about the last token in Bigram, hence we bow have shape (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # we generate probabilities for the next token\n",
    "\n",
    "            # torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) \n",
    "            # where each element is the index of the sampled token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            new_tokens.append(next_token)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model performance, we will use the helper function below.\n",
    "\n",
    "As performance metric we will use perplexity. Perplexity is a metric that measures how well a probability model predicts a sample. It is defined as $2^H$, where $H$ is the cross entropy loss. The lower the perplexity, the better the model.\n",
    "\n",
    "\n",
    "To better understand it, let's recall a few things: \n",
    "\n",
    "LLMs are trained to predict the probability of a word given the previous words. For instance, in the sentence \"She went to the...\", the model predicts the probability of what the next word could be (e.g., store, park, etc.).\n",
    "\n",
    "_Cross entropy_ is a measure of the difference between two probability distributions - in this case, the distribution predicted by the model and the actual distribution of words in the language. A lower cross-entropy means the model's predictions are closer to the actual distribution. We can calculate it as follows:\n",
    "\n",
    "$$H(p,q) = - \\sum_{x} p(x) \\log q(x)$$\n",
    "\n",
    "where $p$ is the actual distribution and $q$ is the predicted distribution. \n",
    "\n",
    "_Perplexity_ can be thought of as the \"effective number of choices\" the model feels it has when making a prediction. A lower perplexity indicates that the model is more confident (or less \"perplexed\") about its predictions.\n",
    "\n",
    "For example, if a model has a perplexity of 10 on a dataset, it means that, on average, each time it tries to predict the next word, it's as uncertain as if it were choosing uniformly and randomly among 10 options. If the perplexity is 100, it's as uncertain as if it were choosing among 100 options, and so on.\n",
    "\n",
    "You can find further information about such metrics [here](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_perplexity(model, data_loader):\n",
    "    # set the model to evaluation mode, i.e., \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        loss = model.loss(x, y)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "    return exp(total_loss / total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "To train the model, we will use a simple training loop and the Adam optimizer.\n",
    "\n",
    "The role of the `Adam` optimizer is to update the parameters of the model using a technique called [mini-batch stochastic gradient descent](http://d2l.ai/chapter_optimization/minibatch-sgd.html).\n",
    "The idea is that we update the weights in the direction of the gradient of the loss function, which we estimate on a small batch of data. The learning rate controls how big the steps are that we take in the direction of the gradient.\n",
    "\n",
    "Setting learning rate is not trivial, you can find more background [here](https://www.jeremyjordan.me/nn-learning-rate/).\n",
    "\n",
    "It is import to remember to use the `zero_grad` function to clear the gradients before computing the gradients for the current batch. Also, remember to call `loss.backward()` to compute the gradients for the current batch.\n",
    "\n",
    "For now, we will use a very simple approach (to reuse our old dataloader) and just predict the second token given the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramModel(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CausalLanguageModelingDataset(train, tokenizer, max_length=40), batch_size=2048, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(CausalLanguageModelingDataset(valid, tokenizer, max_length=40), batch_size=2048)\n",
    "test_loader = torch.utils.data.DataLoader(CausalLanguageModelingDataset(test, tokenizer, max_length=40), batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, lr, eval_every=100):\n",
    "    # set up the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    # start training\n",
    "    # set the model to train mode \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        # iterate over the training data\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            # move the data to the device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = model.loss(x,y)\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the gradients\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print the loss every eval_every iterations\n",
    "            if i % eval_every == 0:\n",
    "                print(f\"Epoch {epoch}, iter {i}, train loss {loss.item():.3f}, val perplexity {estimate_perplexity(model, val_loader):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, train loss 4.230, val perplexity 70.36417\n",
      "Epoch 0, iter 100, train loss 4.113, val perplexity 59.91851\n",
      "Epoch 0, iter 200, train loss 3.938, val perplexity 51.28270\n",
      "Epoch 0, iter 300, train loss 3.814, val perplexity 44.13051\n",
      "Epoch 0, iter 400, train loss 3.669, val perplexity 38.17822\n",
      "Epoch 0, iter 500, train loss 3.507, val perplexity 33.20974\n",
      "Epoch 0, iter 600, train loss 3.376, val perplexity 29.05490\n",
      "Epoch 0, iter 700, train loss 3.237, val perplexity 25.57075\n",
      "Epoch 0, iter 800, train loss 3.111, val perplexity 22.63423\n",
      "Epoch 0, iter 900, train loss 2.975, val perplexity 20.15376\n",
      "Epoch 0, iter 1000, train loss 2.884, val perplexity 18.04662\n",
      "Epoch 0, iter 1100, train loss 2.780, val perplexity 16.25283\n",
      "Epoch 0, iter 1200, train loss 2.706, val perplexity 14.72689\n",
      "Epoch 0, iter 1300, train loss 2.593, val perplexity 13.42251\n",
      "Epoch 0, iter 1400, train loss 2.524, val perplexity 12.30426\n",
      "Epoch 0, iter 1500, train loss 2.430, val perplexity 11.34385\n",
      "Epoch 0, iter 1600, train loss 2.335, val perplexity 10.52085\n",
      "Epoch 0, iter 1700, train loss 2.293, val perplexity 9.81036\n",
      "Epoch 0, iter 1800, train loss 2.247, val perplexity 9.19914\n",
      "Epoch 0, iter 1900, train loss 2.151, val perplexity 8.67190\n",
      "Epoch 0, iter 2000, train loss 2.110, val perplexity 8.21602\n",
      "Epoch 0, iter 2100, train loss 2.023, val perplexity 7.82203\n",
      "Epoch 0, iter 2200, train loss 2.017, val perplexity 7.48051\n",
      "Epoch 0, iter 2300, train loss 1.954, val perplexity 7.18383\n",
      "Epoch 0, iter 2400, train loss 1.910, val perplexity 6.92489\n",
      "Epoch 0, iter 2500, train loss 1.908, val perplexity 6.69931\n",
      "Epoch 0, iter 2600, train loss 1.876, val perplexity 6.50326\n",
      "Epoch 0, iter 2700, train loss 1.895, val perplexity 6.33115\n",
      "Epoch 0, iter 2800, train loss 1.827, val perplexity 6.18042\n",
      "Epoch 0, iter 2900, train loss 1.816, val perplexity 6.04741\n",
      "Epoch 0, iter 3000, train loss 1.747, val perplexity 5.93053\n",
      "Epoch 1, iter 0, train loss 1.820, val perplexity 5.90656\n",
      "Epoch 1, iter 100, train loss 1.802, val perplexity 5.80592\n",
      "Epoch 1, iter 200, train loss 1.740, val perplexity 5.71662\n",
      "Epoch 1, iter 300, train loss 1.774, val perplexity 5.63751\n",
      "Epoch 1, iter 400, train loss 1.722, val perplexity 5.56738\n",
      "Epoch 1, iter 500, train loss 1.709, val perplexity 5.50450\n",
      "Epoch 1, iter 600, train loss 1.728, val perplexity 5.44809\n",
      "Epoch 1, iter 700, train loss 1.644, val perplexity 5.39735\n",
      "Epoch 1, iter 800, train loss 1.643, val perplexity 5.35199\n",
      "Epoch 1, iter 900, train loss 1.654, val perplexity 5.31091\n",
      "Epoch 1, iter 1000, train loss 1.708, val perplexity 5.27410\n",
      "Epoch 1, iter 1100, train loss 1.660, val perplexity 5.24047\n",
      "Epoch 1, iter 1200, train loss 1.645, val perplexity 5.21007\n",
      "Epoch 1, iter 1300, train loss 1.634, val perplexity 5.18242\n",
      "Epoch 1, iter 1400, train loss 1.691, val perplexity 5.15713\n",
      "Epoch 1, iter 1500, train loss 1.674, val perplexity 5.13407\n",
      "Epoch 1, iter 1600, train loss 1.679, val perplexity 5.11296\n",
      "Epoch 1, iter 1700, train loss 1.622, val perplexity 5.09341\n",
      "Epoch 1, iter 1800, train loss 1.600, val perplexity 5.07553\n",
      "Epoch 1, iter 1900, train loss 1.635, val perplexity 5.05899\n",
      "Epoch 1, iter 2000, train loss 1.627, val perplexity 5.04366\n",
      "Epoch 1, iter 2100, train loss 1.626, val perplexity 5.02938\n",
      "Epoch 1, iter 2200, train loss 1.612, val perplexity 5.01611\n",
      "Epoch 1, iter 2300, train loss 1.574, val perplexity 5.00383\n",
      "Epoch 1, iter 2400, train loss 1.619, val perplexity 4.99237\n",
      "Epoch 1, iter 2500, train loss 1.587, val perplexity 4.98171\n",
      "Epoch 1, iter 2600, train loss 1.572, val perplexity 4.97183\n",
      "Epoch 1, iter 2700, train loss 1.595, val perplexity 4.96261\n",
      "Epoch 1, iter 2800, train loss 1.607, val perplexity 4.95387\n",
      "Epoch 1, iter 2900, train loss 1.605, val perplexity 4.94574\n",
      "Epoch 1, iter 3000, train loss 1.613, val perplexity 4.93805\n",
      "Epoch 2, iter 0, train loss 1.607, val perplexity 4.93644\n",
      "Epoch 2, iter 100, train loss 1.645, val perplexity 4.92940\n",
      "Epoch 2, iter 200, train loss 1.599, val perplexity 4.92260\n",
      "Epoch 2, iter 300, train loss 1.569, val perplexity 4.91637\n",
      "Epoch 2, iter 400, train loss 1.655, val perplexity 4.91035\n",
      "Epoch 2, iter 500, train loss 1.596, val perplexity 4.90468\n",
      "Epoch 2, iter 600, train loss 1.601, val perplexity 4.89941\n",
      "Epoch 2, iter 700, train loss 1.560, val perplexity 4.89434\n",
      "Epoch 2, iter 800, train loss 1.607, val perplexity 4.88958\n",
      "Epoch 2, iter 900, train loss 1.575, val perplexity 4.88509\n",
      "Epoch 2, iter 1000, train loss 1.555, val perplexity 4.88082\n",
      "Epoch 2, iter 1100, train loss 1.573, val perplexity 4.87673\n",
      "Epoch 2, iter 1200, train loss 1.585, val perplexity 4.87294\n",
      "Epoch 2, iter 1300, train loss 1.600, val perplexity 4.86928\n",
      "Epoch 2, iter 1400, train loss 1.526, val perplexity 4.86572\n",
      "Epoch 2, iter 1500, train loss 1.603, val perplexity 4.86238\n",
      "Epoch 2, iter 1600, train loss 1.533, val perplexity 4.85920\n",
      "Epoch 2, iter 1700, train loss 1.577, val perplexity 4.85621\n",
      "Epoch 2, iter 1800, train loss 1.577, val perplexity 4.85341\n",
      "Epoch 2, iter 1900, train loss 1.604, val perplexity 4.85070\n",
      "Epoch 2, iter 2000, train loss 1.634, val perplexity 4.84807\n",
      "Epoch 2, iter 2100, train loss 1.572, val perplexity 4.84562\n",
      "Epoch 2, iter 2200, train loss 1.600, val perplexity 4.84327\n",
      "Epoch 2, iter 2300, train loss 1.622, val perplexity 4.84101\n",
      "Epoch 2, iter 2400, train loss 1.591, val perplexity 4.83885\n",
      "Epoch 2, iter 2500, train loss 1.570, val perplexity 4.83682\n",
      "Epoch 2, iter 2600, train loss 1.541, val perplexity 4.83491\n",
      "Epoch 2, iter 2700, train loss 1.557, val perplexity 4.83304\n",
      "Epoch 2, iter 2800, train loss 1.614, val perplexity 4.83126\n",
      "Epoch 2, iter 2900, train loss 1.550, val perplexity 4.82951\n",
      "Epoch 2, iter 3000, train loss 1.595, val perplexity 4.82790\n",
      "Epoch 3, iter 0, train loss 1.621, val perplexity 4.82757\n",
      "Epoch 3, iter 100, train loss 1.596, val perplexity 4.82605\n",
      "Epoch 3, iter 200, train loss 1.628, val perplexity 4.82453\n",
      "Epoch 3, iter 300, train loss 1.571, val perplexity 4.82317\n",
      "Epoch 3, iter 400, train loss 1.596, val perplexity 4.82171\n",
      "Epoch 3, iter 500, train loss 1.542, val perplexity 4.82037\n",
      "Epoch 3, iter 600, train loss 1.547, val perplexity 4.81910\n",
      "Epoch 3, iter 700, train loss 1.539, val perplexity 4.81788\n",
      "Epoch 3, iter 800, train loss 1.594, val perplexity 4.81680\n",
      "Epoch 3, iter 900, train loss 1.610, val perplexity 4.81573\n",
      "Epoch 3, iter 1000, train loss 1.626, val perplexity 4.81471\n",
      "Epoch 3, iter 1100, train loss 1.581, val perplexity 4.81383\n",
      "Epoch 3, iter 1200, train loss 1.561, val perplexity 4.81279\n",
      "Epoch 3, iter 1300, train loss 1.602, val perplexity 4.81187\n",
      "Epoch 3, iter 1400, train loss 1.548, val perplexity 4.81104\n",
      "Epoch 3, iter 1500, train loss 1.559, val perplexity 4.81011\n",
      "Epoch 3, iter 1600, train loss 1.613, val perplexity 4.80929\n",
      "Epoch 3, iter 1700, train loss 1.550, val perplexity 4.80848\n",
      "Epoch 3, iter 1800, train loss 1.599, val perplexity 4.80772\n",
      "Epoch 3, iter 1900, train loss 1.588, val perplexity 4.80702\n",
      "Epoch 3, iter 2000, train loss 1.525, val perplexity 4.80624\n",
      "Epoch 3, iter 2100, train loss 1.589, val perplexity 4.80566\n",
      "Epoch 3, iter 2200, train loss 1.604, val perplexity 4.80508\n",
      "Epoch 3, iter 2300, train loss 1.565, val perplexity 4.80441\n",
      "Epoch 3, iter 2400, train loss 1.598, val perplexity 4.80387\n",
      "Epoch 3, iter 2500, train loss 1.578, val perplexity 4.80331\n",
      "Epoch 3, iter 2600, train loss 1.593, val perplexity 4.80284\n",
      "Epoch 3, iter 2700, train loss 1.561, val perplexity 4.80226\n",
      "Epoch 3, iter 2800, train loss 1.558, val perplexity 4.80170\n",
      "Epoch 3, iter 2900, train loss 1.578, val perplexity 4.80119\n",
      "Epoch 3, iter 3000, train loss 1.604, val perplexity 4.80075\n",
      "Epoch 4, iter 0, train loss 1.568, val perplexity 4.80065\n",
      "Epoch 4, iter 100, train loss 1.544, val perplexity 4.80022\n",
      "Epoch 4, iter 200, train loss 1.563, val perplexity 4.79979\n",
      "Epoch 4, iter 300, train loss 1.586, val perplexity 4.79943\n",
      "Epoch 4, iter 400, train loss 1.531, val perplexity 4.79906\n",
      "Epoch 4, iter 500, train loss 1.576, val perplexity 4.79870\n",
      "Epoch 4, iter 600, train loss 1.552, val perplexity 4.79833\n",
      "Epoch 4, iter 700, train loss 1.567, val perplexity 4.79799\n",
      "Epoch 4, iter 800, train loss 1.552, val perplexity 4.79773\n",
      "Epoch 4, iter 900, train loss 1.611, val perplexity 4.79733\n",
      "Epoch 4, iter 1000, train loss 1.587, val perplexity 4.79721\n",
      "Epoch 4, iter 1100, train loss 1.543, val perplexity 4.79682\n",
      "Epoch 4, iter 1200, train loss 1.587, val perplexity 4.79652\n",
      "Epoch 4, iter 1300, train loss 1.558, val perplexity 4.79630\n",
      "Epoch 4, iter 1400, train loss 1.535, val perplexity 4.79603\n",
      "Epoch 4, iter 1500, train loss 1.541, val perplexity 4.79572\n",
      "Epoch 4, iter 1600, train loss 1.564, val perplexity 4.79556\n",
      "Epoch 4, iter 1700, train loss 1.587, val perplexity 4.79537\n",
      "Epoch 4, iter 1800, train loss 1.597, val perplexity 4.79516\n",
      "Epoch 4, iter 1900, train loss 1.620, val perplexity 4.79496\n",
      "Epoch 4, iter 2000, train loss 1.580, val perplexity 4.79469\n",
      "Epoch 4, iter 2100, train loss 1.552, val perplexity 4.79454\n",
      "Epoch 4, iter 2200, train loss 1.568, val perplexity 4.79439\n",
      "Epoch 4, iter 2300, train loss 1.566, val perplexity 4.79422\n",
      "Epoch 4, iter 2400, train loss 1.599, val perplexity 4.79411\n",
      "Epoch 4, iter 2500, train loss 1.605, val perplexity 4.79389\n",
      "Epoch 4, iter 2600, train loss 1.562, val perplexity 4.79378\n",
      "Epoch 4, iter 2700, train loss 1.573, val perplexity 4.79364\n",
      "Epoch 4, iter 2800, train loss 1.623, val perplexity 4.79343\n",
      "Epoch 4, iter 2900, train loss 1.583, val perplexity 4.79329\n",
      "Epoch 4, iter 3000, train loss 1.583, val perplexity 4.79320\n",
      "Epoch 5, iter 0, train loss 1.583, val perplexity 4.79321\n",
      "Epoch 5, iter 100, train loss 1.567, val perplexity 4.79309\n",
      "Epoch 5, iter 200, train loss 1.578, val perplexity 4.79299\n",
      "Epoch 5, iter 300, train loss 1.526, val perplexity 4.79285\n",
      "Epoch 5, iter 400, train loss 1.587, val perplexity 4.79277\n",
      "Epoch 5, iter 500, train loss 1.557, val perplexity 4.79265\n",
      "Epoch 5, iter 600, train loss 1.621, val perplexity 4.79255\n",
      "Epoch 5, iter 700, train loss 1.545, val perplexity 4.79242\n",
      "Epoch 5, iter 800, train loss 1.586, val perplexity 4.79233\n",
      "Epoch 5, iter 900, train loss 1.581, val perplexity 4.79228\n",
      "Epoch 5, iter 1000, train loss 1.608, val perplexity 4.79213\n",
      "Epoch 5, iter 1100, train loss 1.563, val perplexity 4.79210\n",
      "Epoch 5, iter 1200, train loss 1.550, val perplexity 4.79210\n",
      "Epoch 5, iter 1300, train loss 1.550, val perplexity 4.79198\n",
      "Epoch 5, iter 1400, train loss 1.604, val perplexity 4.79193\n",
      "Epoch 5, iter 1500, train loss 1.607, val perplexity 4.79173\n",
      "Epoch 5, iter 1600, train loss 1.589, val perplexity 4.79158\n",
      "Epoch 5, iter 1700, train loss 1.563, val perplexity 4.79152\n",
      "Epoch 5, iter 1800, train loss 1.579, val perplexity 4.79146\n",
      "Epoch 5, iter 1900, train loss 1.580, val perplexity 4.79140\n",
      "Epoch 5, iter 2000, train loss 1.584, val perplexity 4.79148\n",
      "Epoch 5, iter 2100, train loss 1.581, val perplexity 4.79142\n",
      "Epoch 5, iter 2200, train loss 1.609, val perplexity 4.79131\n",
      "Epoch 5, iter 2300, train loss 1.565, val perplexity 4.79128\n",
      "Epoch 5, iter 2400, train loss 1.501, val perplexity 4.79122\n",
      "Epoch 5, iter 2500, train loss 1.536, val perplexity 4.79123\n",
      "Epoch 5, iter 2600, train loss 1.609, val perplexity 4.79108\n",
      "Epoch 5, iter 2700, train loss 1.578, val perplexity 4.79103\n",
      "Epoch 5, iter 2800, train loss 1.526, val perplexity 4.79096\n",
      "Epoch 5, iter 2900, train loss 1.509, val perplexity 4.79099\n",
      "Epoch 5, iter 3000, train loss 1.559, val perplexity 4.79101\n",
      "Epoch 6, iter 0, train loss 1.582, val perplexity 4.79101\n",
      "Epoch 6, iter 100, train loss 1.556, val perplexity 4.79101\n",
      "Epoch 6, iter 200, train loss 1.585, val perplexity 4.79088\n",
      "Epoch 6, iter 300, train loss 1.531, val perplexity 4.79094\n",
      "Epoch 6, iter 400, train loss 1.552, val perplexity 4.79084\n",
      "Epoch 6, iter 500, train loss 1.536, val perplexity 4.79081\n",
      "Epoch 6, iter 600, train loss 1.536, val perplexity 4.79068\n",
      "Epoch 6, iter 700, train loss 1.594, val perplexity 4.79068\n",
      "Epoch 6, iter 800, train loss 1.559, val perplexity 4.79071\n",
      "Epoch 6, iter 900, train loss 1.582, val perplexity 4.79064\n",
      "Epoch 6, iter 1000, train loss 1.530, val perplexity 4.79074\n",
      "Epoch 6, iter 1100, train loss 1.531, val perplexity 4.79060\n",
      "Epoch 6, iter 1200, train loss 1.558, val perplexity 4.79057\n",
      "Epoch 6, iter 1300, train loss 1.566, val perplexity 4.79051\n",
      "Epoch 6, iter 1400, train loss 1.562, val perplexity 4.79054\n",
      "Epoch 6, iter 1500, train loss 1.602, val perplexity 4.79058\n",
      "Epoch 6, iter 1600, train loss 1.545, val perplexity 4.79039\n",
      "Epoch 6, iter 1700, train loss 1.556, val perplexity 4.79039\n",
      "Epoch 6, iter 1800, train loss 1.560, val perplexity 4.79046\n",
      "Epoch 6, iter 1900, train loss 1.548, val perplexity 4.79052\n",
      "Epoch 6, iter 2000, train loss 1.542, val perplexity 4.79049\n",
      "Epoch 6, iter 2100, train loss 1.616, val perplexity 4.79052\n",
      "Epoch 6, iter 2200, train loss 1.535, val perplexity 4.79047\n",
      "Epoch 6, iter 2300, train loss 1.513, val perplexity 4.79043\n",
      "Epoch 6, iter 2400, train loss 1.568, val perplexity 4.79038\n",
      "Epoch 6, iter 2500, train loss 1.588, val perplexity 4.79045\n",
      "Epoch 6, iter 2600, train loss 1.595, val perplexity 4.79037\n",
      "Epoch 6, iter 2700, train loss 1.532, val perplexity 4.79027\n",
      "Epoch 6, iter 2800, train loss 1.592, val perplexity 4.79032\n",
      "Epoch 6, iter 2900, train loss 1.588, val perplexity 4.79033\n",
      "Epoch 6, iter 3000, train loss 1.579, val perplexity 4.79030\n",
      "Epoch 7, iter 0, train loss 1.577, val perplexity 4.79030\n",
      "Epoch 7, iter 100, train loss 1.508, val perplexity 4.79027\n",
      "Epoch 7, iter 200, train loss 1.564, val perplexity 4.79026\n",
      "Epoch 7, iter 300, train loss 1.560, val perplexity 4.79019\n",
      "Epoch 7, iter 400, train loss 1.616, val perplexity 4.79022\n",
      "Epoch 7, iter 500, train loss 1.549, val perplexity 4.79025\n",
      "Epoch 7, iter 600, train loss 1.596, val perplexity 4.79029\n",
      "Epoch 7, iter 700, train loss 1.531, val perplexity 4.79021\n",
      "Epoch 7, iter 800, train loss 1.553, val perplexity 4.79026\n",
      "Epoch 7, iter 900, train loss 1.603, val perplexity 4.79031\n",
      "Epoch 7, iter 1000, train loss 1.582, val perplexity 4.79037\n",
      "Epoch 7, iter 1100, train loss 1.572, val perplexity 4.79019\n",
      "Epoch 7, iter 1200, train loss 1.580, val perplexity 4.79016\n",
      "Epoch 7, iter 1300, train loss 1.519, val perplexity 4.79022\n",
      "Epoch 7, iter 1400, train loss 1.601, val perplexity 4.79024\n",
      "Epoch 7, iter 1500, train loss 1.561, val perplexity 4.79025\n",
      "Epoch 7, iter 1600, train loss 1.585, val perplexity 4.79014\n",
      "Epoch 7, iter 1700, train loss 1.578, val perplexity 4.79014\n",
      "Epoch 7, iter 1800, train loss 1.592, val perplexity 4.79017\n",
      "Epoch 7, iter 1900, train loss 1.575, val perplexity 4.79009\n",
      "Epoch 7, iter 2000, train loss 1.535, val perplexity 4.79006\n",
      "Epoch 7, iter 2100, train loss 1.557, val perplexity 4.79002\n",
      "Epoch 7, iter 2200, train loss 1.570, val perplexity 4.79008\n",
      "Epoch 7, iter 2300, train loss 1.607, val perplexity 4.79005\n",
      "Epoch 7, iter 2400, train loss 1.588, val perplexity 4.79003\n",
      "Epoch 7, iter 2500, train loss 1.583, val perplexity 4.79006\n",
      "Epoch 7, iter 2600, train loss 1.554, val perplexity 4.78999\n",
      "Epoch 7, iter 2700, train loss 1.524, val perplexity 4.79006\n",
      "Epoch 7, iter 2800, train loss 1.590, val perplexity 4.79002\n",
      "Epoch 7, iter 2900, train loss 1.545, val perplexity 4.79001\n",
      "Epoch 7, iter 3000, train loss 1.549, val perplexity 4.79006\n",
      "Epoch 8, iter 0, train loss 1.558, val perplexity 4.79007\n",
      "Epoch 8, iter 100, train loss 1.572, val perplexity 4.79002\n",
      "Epoch 8, iter 200, train loss 1.586, val perplexity 4.79002\n",
      "Epoch 8, iter 300, train loss 1.535, val perplexity 4.79004\n",
      "Epoch 8, iter 400, train loss 1.564, val perplexity 4.79012\n",
      "Epoch 8, iter 500, train loss 1.560, val perplexity 4.79005\n",
      "Epoch 8, iter 600, train loss 1.581, val perplexity 4.79013\n",
      "Epoch 8, iter 700, train loss 1.596, val perplexity 4.79022\n",
      "Epoch 8, iter 800, train loss 1.493, val perplexity 4.79019\n",
      "Epoch 8, iter 900, train loss 1.548, val perplexity 4.79015\n",
      "Epoch 8, iter 1000, train loss 1.579, val perplexity 4.79009\n",
      "Epoch 8, iter 1100, train loss 1.613, val perplexity 4.79015\n",
      "Epoch 8, iter 1200, train loss 1.563, val perplexity 4.79004\n",
      "Epoch 8, iter 1300, train loss 1.577, val perplexity 4.79001\n",
      "Epoch 8, iter 1400, train loss 1.542, val perplexity 4.78997\n",
      "Epoch 8, iter 1500, train loss 1.574, val perplexity 4.78996\n",
      "Epoch 8, iter 1600, train loss 1.562, val perplexity 4.79002\n",
      "Epoch 8, iter 1700, train loss 1.553, val perplexity 4.79003\n",
      "Epoch 8, iter 1800, train loss 1.589, val perplexity 4.79001\n",
      "Epoch 8, iter 1900, train loss 1.600, val perplexity 4.79004\n",
      "Epoch 8, iter 2000, train loss 1.568, val perplexity 4.79003\n",
      "Epoch 8, iter 2100, train loss 1.546, val perplexity 4.79005\n",
      "Epoch 8, iter 2200, train loss 1.581, val perplexity 4.78993\n",
      "Epoch 8, iter 2300, train loss 1.536, val perplexity 4.78989\n",
      "Epoch 8, iter 2400, train loss 1.549, val perplexity 4.78996\n",
      "Epoch 8, iter 2500, train loss 1.554, val perplexity 4.79000\n",
      "Epoch 8, iter 2600, train loss 1.607, val perplexity 4.78994\n",
      "Epoch 8, iter 2700, train loss 1.575, val perplexity 4.78994\n",
      "Epoch 8, iter 2800, train loss 1.560, val perplexity 4.78990\n",
      "Epoch 8, iter 2900, train loss 1.587, val perplexity 4.78994\n",
      "Epoch 8, iter 3000, train loss 1.577, val perplexity 4.78990\n",
      "Epoch 9, iter 0, train loss 1.549, val perplexity 4.78991\n",
      "Epoch 9, iter 100, train loss 1.565, val perplexity 4.78990\n",
      "Epoch 9, iter 200, train loss 1.568, val perplexity 4.79006\n",
      "Epoch 9, iter 300, train loss 1.546, val perplexity 4.79004\n",
      "Epoch 9, iter 400, train loss 1.526, val perplexity 4.78991\n",
      "Epoch 9, iter 500, train loss 1.622, val perplexity 4.78986\n",
      "Epoch 9, iter 600, train loss 1.542, val perplexity 4.78990\n",
      "Epoch 9, iter 700, train loss 1.600, val perplexity 4.78993\n",
      "Epoch 9, iter 800, train loss 1.583, val perplexity 4.78996\n",
      "Epoch 9, iter 900, train loss 1.541, val perplexity 4.78992\n",
      "Epoch 9, iter 1000, train loss 1.575, val perplexity 4.78989\n",
      "Epoch 9, iter 1100, train loss 1.590, val perplexity 4.78991\n",
      "Epoch 9, iter 1200, train loss 1.503, val perplexity 4.78993\n",
      "Epoch 9, iter 1300, train loss 1.579, val perplexity 4.78996\n",
      "Epoch 9, iter 1400, train loss 1.595, val perplexity 4.79001\n",
      "Epoch 9, iter 1500, train loss 1.573, val perplexity 4.79003\n",
      "Epoch 9, iter 1600, train loss 1.579, val perplexity 4.79004\n",
      "Epoch 9, iter 1700, train loss 1.587, val perplexity 4.79009\n",
      "Epoch 9, iter 1800, train loss 1.568, val perplexity 4.79011\n",
      "Epoch 9, iter 1900, train loss 1.575, val perplexity 4.78999\n",
      "Epoch 9, iter 2000, train loss 1.536, val perplexity 4.78995\n",
      "Epoch 9, iter 2100, train loss 1.506, val perplexity 4.78989\n",
      "Epoch 9, iter 2200, train loss 1.555, val perplexity 4.78992\n",
      "Epoch 9, iter 2300, train loss 1.560, val perplexity 4.78995\n",
      "Epoch 9, iter 2400, train loss 1.544, val perplexity 4.78995\n",
      "Epoch 9, iter 2500, train loss 1.550, val perplexity 4.78988\n",
      "Epoch 9, iter 2600, train loss 1.520, val perplexity 4.78984\n",
      "Epoch 9, iter 2700, train loss 1.580, val perplexity 4.78992\n",
      "Epoch 9, iter 2800, train loss 1.586, val perplexity 4.78992\n",
      "Epoch 9, iter 2900, train loss 1.590, val perplexity 4.78994\n",
      "Epoch 9, iter 3000, train loss 1.626, val perplexity 4.78993\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, epochs=10, lr=1e-3, eval_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the model by generating new SMILES strings. We will start with a random token and generate 100 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[4]])\n",
    "a = a.to(device)\n",
    "generation = model.generate(a, max_new_tokens=30).cpu().numpy()\n",
    "smiles = tokenizer.decode(generation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[O-])N(Cnc1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:01:00] SMILES Parse Error: extra close parentheses while parsing: [O-])N(Cnc1\n",
      "[11:01:00] SMILES Parse Error: Failed parsing SMILES '[O-])N(Cnc1' for input: '[O-])N(Cnc1'\n"
     ]
    }
   ],
   "source": [
    "Chem.MolFromSmiles(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look too bad, but we can do better (if you would run the code multiple times, you would see that the results are not always a valid SMILES)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making tokens talk using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our bigram models we made predictions based on the previous word.\n",
    "This is clearly not enough to make good predictions.\n",
    "We can improve our model by taking into more past tokens into account.\n",
    "\n",
    "One naïve way to incorporate more context into our model might be to simply \"pool\" (features of) the preceding tokens.\n",
    "This kind of pooling is similar to what we do in GNNs, e.g., to combine node embeddings. \n",
    "\n",
    "A very simple pooling operation is the average of the embeddings of the preceding tokens. Later, when we will implement self-attention, we will not use a simple average, but a special weighted average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 2, 5, 3 # batch size, time (sequence length), channels (features)\n",
    "\n",
    "# create random data of shape (B, T, C)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b,:t+1] # shape (t, C)\n",
    "        \n",
    "        x_bag_of_words[b, t] = torch.mean(x_prev, dim=0) # shape (C,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: Causal masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nested for loop for averaging is slow. However, we can implement this in an efficient way if we observe a few things: \n",
    "\n",
    "- If we want to predict next tokens, we do not want to let the future tokens influence the prediction. \n",
    "Therefore, we can use a so-called causal mask to mask out the future tokens.\n",
    "\n",
    "- A matrix multiplication can be thought of as a weighted sum of the rows of the matrix, where the weights are given by the columns of the matrix. This is easy to see if we think of the following extremes: \n",
    "\n",
    "  - We can compute the sum of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones.\n",
    "   - We can compute the mean of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones and dividing by the number of ones in the lower-triangular matrix.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch` we can use `tril` to create a lower-triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_triangular_mask = torch.tril(torch.ones((T,T)))\n",
    "\n",
    "weight = torch.ones((T,T))\n",
    "weight = torch.masked_fill(weight, lower_triangular_mask==0, float('-inf'))\n",
    "\n",
    "weight = torch.softmax(weight, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the softmax function to normalize the weights in the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7405, -1.5553,  0.0071],\n",
       "         [ 0.1939,  0.2470,  0.8337],\n",
       "         [-0.4967,  0.2113,  0.8790],\n",
       "         [-0.3595,  0.5234,  0.3761],\n",
       "         [-0.1104,  0.7610,  0.3781]],\n",
       "\n",
       "        [[-0.2511, -0.1353,  0.4621],\n",
       "         [-0.9781,  0.3923, -0.3655],\n",
       "         [-0.7804,  0.4323,  0.0366],\n",
       "         [-0.5360, -0.0959, -0.1145],\n",
       "         [-0.6021, -0.4834, -0.2511]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention as special weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple average we used above, all past tokens were treated equally.\n",
    "However, it might be useful to _pay more attention_  to certain tokens than to others. That is, we want to gather information from the past -- but do this in a data-dependent way.\n",
    "The attention mechanism allows us to do this.\n",
    "\n",
    "The attention mechanism does this by having a query vector $q$ and a key vector $k$ for each token. We then define \"similarity\" or \"relevance\" between two tokens $i$ and $j$ as the dot product between their query and key vectors, which we derive from the embeddings of the tokens by multiplying them with the learnable weight matrices $W_q$ and $W_k$.\n",
    "\n",
    "$$\n",
    "\\text{sim}(i, j) = a(i, h) = q_ik_j^T = \\text{emb}_i W_q W_k^T \\text{emb}_j^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this gives us now a way to refine the `weight_matrix` we used above. Instead of weighting all tokens equally, we can now learn a weight matrix that tells us how much attention to pay to each token.\n",
    "\n",
    "To start the implementation, we will first derive query and key vectors from the embeddings. We will then compute the similarity matrix and apply the softmax function to normalize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 2, 5, 3 # batch size, time (sequence length), channels (features)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16 # hyperparameter\n",
    "\n",
    "# with bias = False, it only perform matrix multiplication\n",
    "key_layer = nn.Linear(C, head_size, bias=False)  \n",
    "query_layer = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention matrix defined above is now a simple matrix multiplication between the query and key vectors. The attention matrix is then normalized using a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_layer(x) # shape (B, T, head_size)\n",
    "key = key_layer(x) # shape (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = query @ key.transpose(1,2) # shape (B, T, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of the attention matrix is (B, T, T). The attention matrix is a matrix where each row corresponds to a query and each column corresponds to a key. The value at position (i, j) in the attention matrix is the attention score between the i-th query and the j-th key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7304,  1.8198, -1.8203,  0.1634,  3.3620],\n",
       "         [-0.7860,  0.2509, -0.0787,  0.1417,  0.3819],\n",
       "         [-0.0146, -0.7588,  0.6548, -0.1753, -1.3697],\n",
       "         [ 0.2951,  0.2504, -0.2850,  0.0583,  0.5083],\n",
       "         [-0.6207,  0.9888, -0.7507,  0.3115,  1.7422]],\n",
       "\n",
       "        [[ 2.5424,  0.3088,  0.4298,  0.7943,  2.0868],\n",
       "         [ 0.7896,  0.1249,  0.2086,  0.2485,  0.4375],\n",
       "         [ 1.5647,  0.2457,  0.4105,  0.5208,  0.9036],\n",
       "         [-0.8603, -0.3990, -0.8800,  0.1462,  1.7874],\n",
       "         [-2.7816, -0.7904, -1.6195, -0.5436,  1.2935]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to avoid the future tokens to influence the prediction, we will use a causal mask.\n",
    "We do this the same way as we did above, by using `torch.tril`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_triangular_mask = torch.tril(torch.ones((T,T)))\n",
    "\n",
    "attention = torch.masked_fill(attention, lower_triangular_mask==0, float('-inf'))   \n",
    "\n",
    "attention = torch.softmax(attention, dim=2) # shape (B, T, T), softmax along the last dimension\n",
    "\n",
    "out = attention @ x # shape (B, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the attention mechanism popularized in the [\"attention is all you need\" paper](https://arxiv.org/abs/1706.03762) we add even more expressive power by transforming `x` before we multiply it with the attention matrix. We call this transformed `x` the value vector (or matrix). The full implementation of the attention mechanism is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 2, 5, 3 # batch size, time (sequence length), channels (features)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16 # hyperparameter\n",
    "\n",
    "# what do I contain\n",
    "# with bias = False, it only perform matrix multiplication\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# what am I looking for\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# what I will tell you\n",
    "value = nn.Linear(C, head_size, bias=False) # Output: (B, T, head_size)\n",
    "# self-attention because k, q, v come all from the same input\n",
    "k = key(x) # shape (B, T, head_size)\n",
    "q = query(x) # shape (B, T, head_size)\n",
    "v = value(x) # shape (B, T, head_size)\n",
    "\n",
    "# now, we want to compute the attention\n",
    "# we need to compute the dot product between k and q\n",
    "weight_matrix = q @ k.transpose(-2, -1) # shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "\n",
    "# now we add the masking\n",
    "# we want to mask out the future\n",
    "# this is what is known as \"decoder\" block \n",
    "lower_triangular = torch.tril(torch.ones((T,T)))\n",
    "weight_matrix = weight_matrix.masked_fill(lower_triangular==0, float('-inf'))\n",
    "\n",
    "# use softmax to normalize\n",
    "weight_matrix = torch.softmax(weight_matrix, dim=-1)/np.sqrt(head_size) # shape (B, T, T)\n",
    "\n",
    "out = weight_matrix @ v # shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interlude: Why do we divide by sqrt(head_size) in the self-attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used one more trick to make the training more stable. We scaled the weight_matrix by the square root of the head_size. [This is because the variance of the dot product is proportional to the dimensionality of the vectors.](https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled). Not scaling the weight matrix can lead to numerical instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this, let's run a quick experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "dimensions = [1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "for d in dimensions:\n",
    "\n",
    "    k = torch.randn(B, T, d)\n",
    "    q = torch.randn(B, T, d)\n",
    "\n",
    "    # compute the batched matrix product between k and q\n",
    "    weight_matrix = torch.bmm(q, k.transpose(-2, -1))   # shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "    variances.append(weight_matrix.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Variance')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dimensions, variances)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Dimensionality')\n",
    "plt.ylabel('Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has an important impact when we apply `softmax`.\n",
    "Positive and negative \"outliers\" will be \"sequeezed\" to 1 and 0. You can test this by creating a 1D tensor (`a`) and applying softmax on it. \n",
    "Then multiply the values in the tensor  (`a`) and again apply softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652]) tensor([0.0000e+00, 3.7835e-44, 1.0000e+00])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125/1895642280.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(torch.tensor([1.,2.,3.])),F.softmax(torch.tensor([1.,2.,3.])*100) )\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(torch.tensor([1.,2.,3.])),F.softmax(torch.tensor([1.,2.,3.])*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written as a formula, the attention mechanism is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where $Q$ is the query matrix, $K$ is the key matrix, and $V$ is the value matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring into a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, block_size, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        \n",
    "        self.register_buffer('lower_triangular', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x): \n",
    "        B, T, C  = x.shape\n",
    "        key = self.key(x)\n",
    "        query = self.query(x) # B, T, head\n",
    "        value = self.value(x)   # B, T, head\n",
    "\n",
    "        weight_matrix = query @ key.transpose(-2, -1) * C ** (-0.5) # shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "        weight_matrix = weight_matrix.masked_fill(self.lower_triangular[:T, :T].logical_not(), float('-inf'))\n",
    "        weight_matrix = F.softmax(weight_matrix, dim=-1)\n",
    "\n",
    "        out = weight_matrix @ value # shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revamped Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use it to \"refine\" our bigram model. \n",
    "We will additionally also perform two more changes:\n",
    "\n",
    "- we will add positional embeddings: We will add the positional embeddings to the input embeddings. This will allow the model to take into account the position of the tokens in the sequence.\n",
    "- we will add one more indirection: One simple way of improving the expressiveness is to add one linear layer. While in the bigram model we only had one embedding layer (that mapped inputs of size `vocab_size` to `vocab_size`), we can now change the embedding layer to map inputs of size `vocab_size` to `embedding_size`. We can then add a linear layer that maps inputs of size `embedding_size` to `vocab_size`. This way, we can learn a more complex mapping from the embeddings to the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length=100, head_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # map the input ids to embeddings\n",
    "        self.token_embedding =  nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # add positional embeddings (each position has its own learnable embedding vector)\n",
    "        self.positional_embedding = nn.Embedding(sequence_length, embedding_dim)\n",
    "\n",
    "        # the self-attention layer\n",
    "        self.attention = Head(embedding_dim, sequence_length, head_size)\n",
    "\n",
    "        # the linear layer that maps the output of the self-attention layer to the vocabulary size\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        # store the sequence length\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.token_embedding(x) # B, T, C \n",
    "        x += self.positional_embedding(torch.arange(T, device=device)) # B, T, C\n",
    "        x = self.attention(x) # B, T, head_size\n",
    "        x = self.lm_head(x) # B, T, vocab_size\n",
    "        # The prediction is for each token a probability distribution over the vocabulary\n",
    "        # this indicates how likely each token is the next token\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        logits = self.forward(x) # (B, T, C)\n",
    "        B, T, C = logits.shape\n",
    "\n",
    "        # Note that that the implementation below is because of how we - for educational purposes - have defined the dataset\n",
    "        # A better way is to have inputs and outputs of the same length (and to not manually code the sliding window\n",
    "        # but to instead use a causal mask)\n",
    "        logits = logits[:, -1, :] # we only care about the last token \n",
    "        logits = logits.view(B, C)\n",
    "        y = y.view(B)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens=100):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        # we generate max_new_tokens new tokens\n",
    "        for _t in range(max_new_tokens):\n",
    "            logits = self.forward(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, train loss 3.922, val perplexity 48.23215\n",
      "Epoch 0, iter 100, train loss 1.893, val perplexity 6.40684\n",
      "Epoch 0, iter 200, train loss 1.555, val perplexity 4.69242\n",
      "Epoch 0, iter 300, train loss 1.458, val perplexity 4.32219\n",
      "Epoch 0, iter 400, train loss 1.410, val perplexity 4.01232\n",
      "Epoch 0, iter 500, train loss 1.372, val perplexity 3.83476\n",
      "Epoch 0, iter 600, train loss 1.279, val perplexity 3.76709\n",
      "Epoch 0, iter 700, train loss 1.302, val perplexity 3.70180\n",
      "Epoch 0, iter 800, train loss 1.279, val perplexity 3.64416\n",
      "Epoch 0, iter 900, train loss 1.261, val perplexity 3.58456\n",
      "Epoch 0, iter 1000, train loss 1.266, val perplexity 3.56547\n",
      "Epoch 0, iter 1100, train loss 1.279, val perplexity 3.53003\n",
      "Epoch 0, iter 1200, train loss 1.321, val perplexity 3.52294\n",
      "Epoch 0, iter 1300, train loss 1.341, val perplexity 3.50695\n",
      "Epoch 0, iter 1400, train loss 1.315, val perplexity 3.48702\n",
      "Epoch 0, iter 1500, train loss 1.241, val perplexity 3.46926\n",
      "Epoch 0, iter 1600, train loss 1.278, val perplexity 3.46673\n",
      "Epoch 0, iter 1700, train loss 1.263, val perplexity 3.46404\n",
      "Epoch 0, iter 1800, train loss 1.235, val perplexity 3.43467\n",
      "Epoch 0, iter 1900, train loss 1.253, val perplexity 3.44030\n",
      "Epoch 0, iter 2000, train loss 1.216, val perplexity 3.42769\n",
      "Epoch 0, iter 2100, train loss 1.246, val perplexity 3.41077\n",
      "Epoch 0, iter 2200, train loss 1.220, val perplexity 3.40553\n",
      "Epoch 0, iter 2300, train loss 1.248, val perplexity 3.40351\n",
      "Epoch 0, iter 2400, train loss 1.252, val perplexity 3.39617\n",
      "Epoch 0, iter 2500, train loss 1.238, val perplexity 3.38495\n",
      "Epoch 0, iter 2600, train loss 1.205, val perplexity 3.37821\n",
      "Epoch 0, iter 2700, train loss 1.204, val perplexity 3.36739\n",
      "Epoch 0, iter 2800, train loss 1.280, val perplexity 3.36195\n",
      "Epoch 0, iter 2900, train loss 1.216, val perplexity 3.36127\n",
      "Epoch 0, iter 3000, train loss 1.226, val perplexity 3.37020\n",
      "Epoch 1, iter 0, train loss 1.170, val perplexity 3.36914\n",
      "Epoch 1, iter 100, train loss 1.261, val perplexity 3.36730\n",
      "Epoch 1, iter 200, train loss 1.173, val perplexity 3.34826\n",
      "Epoch 1, iter 300, train loss 1.219, val perplexity 3.34976\n",
      "Epoch 1, iter 400, train loss 1.215, val perplexity 3.34097\n",
      "Epoch 1, iter 500, train loss 1.259, val perplexity 3.34095\n",
      "Epoch 1, iter 600, train loss 1.160, val perplexity 3.34483\n",
      "Epoch 1, iter 700, train loss 1.168, val perplexity 3.34927\n",
      "Epoch 1, iter 800, train loss 1.174, val perplexity 3.33263\n",
      "Epoch 1, iter 900, train loss 1.180, val perplexity 3.31691\n",
      "Epoch 1, iter 1000, train loss 1.186, val perplexity 3.31930\n",
      "Epoch 1, iter 1100, train loss 1.217, val perplexity 3.30763\n",
      "Epoch 1, iter 1200, train loss 1.172, val perplexity 3.31444\n",
      "Epoch 1, iter 1300, train loss 1.221, val perplexity 3.29735\n",
      "Epoch 1, iter 1400, train loss 1.196, val perplexity 3.30116\n",
      "Epoch 1, iter 1500, train loss 1.176, val perplexity 3.30627\n",
      "Epoch 1, iter 1600, train loss 1.195, val perplexity 3.29642\n",
      "Epoch 1, iter 1700, train loss 1.195, val perplexity 3.30111\n",
      "Epoch 1, iter 1800, train loss 1.174, val perplexity 3.29289\n",
      "Epoch 1, iter 1900, train loss 1.200, val perplexity 3.27747\n",
      "Epoch 1, iter 2000, train loss 1.177, val perplexity 3.27306\n",
      "Epoch 1, iter 2100, train loss 1.161, val perplexity 3.28105\n",
      "Epoch 1, iter 2200, train loss 1.208, val perplexity 3.27101\n",
      "Epoch 1, iter 2300, train loss 1.206, val perplexity 3.26604\n",
      "Epoch 1, iter 2400, train loss 1.185, val perplexity 3.26219\n",
      "Epoch 1, iter 2500, train loss 1.173, val perplexity 3.27041\n",
      "Epoch 1, iter 2600, train loss 1.184, val perplexity 3.25489\n",
      "Epoch 1, iter 2700, train loss 1.190, val perplexity 3.24876\n",
      "Epoch 1, iter 2800, train loss 1.147, val perplexity 3.25139\n",
      "Epoch 1, iter 2900, train loss 1.163, val perplexity 3.25524\n",
      "Epoch 1, iter 3000, train loss 1.184, val perplexity 3.25733\n",
      "Epoch 2, iter 0, train loss 1.210, val perplexity 3.25340\n",
      "Epoch 2, iter 100, train loss 1.186, val perplexity 3.23626\n",
      "Epoch 2, iter 200, train loss 1.175, val perplexity 3.23887\n",
      "Epoch 2, iter 300, train loss 1.176, val perplexity 3.26533\n",
      "Epoch 2, iter 400, train loss 1.194, val perplexity 3.23699\n",
      "Epoch 2, iter 500, train loss 1.176, val perplexity 3.23171\n",
      "Epoch 2, iter 600, train loss 1.147, val perplexity 3.23089\n",
      "Epoch 2, iter 700, train loss 1.164, val perplexity 3.22644\n",
      "Epoch 2, iter 800, train loss 1.217, val perplexity 3.22565\n",
      "Epoch 2, iter 900, train loss 1.198, val perplexity 3.24317\n",
      "Epoch 2, iter 1000, train loss 1.173, val perplexity 3.21558\n",
      "Epoch 2, iter 1100, train loss 1.139, val perplexity 3.21572\n",
      "Epoch 2, iter 1200, train loss 1.205, val perplexity 3.22149\n",
      "Epoch 2, iter 1300, train loss 1.152, val perplexity 3.20762\n",
      "Epoch 2, iter 1400, train loss 1.132, val perplexity 3.20646\n",
      "Epoch 2, iter 1500, train loss 1.139, val perplexity 3.20836\n",
      "Epoch 2, iter 1600, train loss 1.178, val perplexity 3.19696\n",
      "Epoch 2, iter 1700, train loss 1.151, val perplexity 3.21573\n",
      "Epoch 2, iter 1800, train loss 1.192, val perplexity 3.19919\n",
      "Epoch 2, iter 1900, train loss 1.160, val perplexity 3.20343\n",
      "Epoch 2, iter 2000, train loss 1.204, val perplexity 3.21099\n",
      "Epoch 2, iter 2100, train loss 1.158, val perplexity 3.19490\n",
      "Epoch 2, iter 2200, train loss 1.129, val perplexity 3.18867\n",
      "Epoch 2, iter 2300, train loss 1.190, val perplexity 3.20488\n",
      "Epoch 2, iter 2400, train loss 1.188, val perplexity 3.18957\n",
      "Epoch 2, iter 2500, train loss 1.159, val perplexity 3.19335\n",
      "Epoch 2, iter 2600, train loss 1.208, val perplexity 3.18443\n",
      "Epoch 2, iter 2700, train loss 1.159, val perplexity 3.19327\n",
      "Epoch 2, iter 2800, train loss 1.161, val perplexity 3.18515\n",
      "Epoch 2, iter 2900, train loss 1.153, val perplexity 3.18089\n",
      "Epoch 2, iter 3000, train loss 1.175, val perplexity 3.17606\n",
      "Epoch 3, iter 0, train loss 1.198, val perplexity 3.18506\n",
      "Epoch 3, iter 100, train loss 1.155, val perplexity 3.19169\n",
      "Epoch 3, iter 200, train loss 1.126, val perplexity 3.17504\n",
      "Epoch 3, iter 300, train loss 1.182, val perplexity 3.18656\n",
      "Epoch 3, iter 400, train loss 1.212, val perplexity 3.17432\n",
      "Epoch 3, iter 500, train loss 1.185, val perplexity 3.19128\n",
      "Epoch 3, iter 600, train loss 1.109, val perplexity 3.17049\n",
      "Epoch 3, iter 700, train loss 1.156, val perplexity 3.18052\n",
      "Epoch 3, iter 800, train loss 1.141, val perplexity 3.18656\n",
      "Epoch 3, iter 900, train loss 1.206, val perplexity 3.17240\n",
      "Epoch 3, iter 1000, train loss 1.162, val perplexity 3.14512\n",
      "Epoch 3, iter 1100, train loss 1.100, val perplexity 3.13509\n",
      "Epoch 3, iter 1200, train loss 1.105, val perplexity 3.13958\n",
      "Epoch 3, iter 1300, train loss 1.148, val perplexity 3.13628\n",
      "Epoch 3, iter 1400, train loss 1.161, val perplexity 3.13081\n",
      "Epoch 3, iter 1500, train loss 1.117, val perplexity 3.12571\n",
      "Epoch 3, iter 1600, train loss 1.149, val perplexity 3.14111\n",
      "Epoch 3, iter 1700, train loss 1.113, val perplexity 3.12275\n",
      "Epoch 3, iter 1800, train loss 1.186, val perplexity 3.11286\n",
      "Epoch 3, iter 1900, train loss 1.152, val perplexity 3.10916\n",
      "Epoch 3, iter 2000, train loss 1.139, val perplexity 3.10819\n",
      "Epoch 3, iter 2100, train loss 1.138, val perplexity 3.12487\n",
      "Epoch 3, iter 2200, train loss 1.106, val perplexity 3.10011\n",
      "Epoch 3, iter 2300, train loss 1.116, val perplexity 3.09611\n",
      "Epoch 3, iter 2400, train loss 1.132, val perplexity 3.09989\n",
      "Epoch 3, iter 2500, train loss 1.119, val perplexity 3.09307\n",
      "Epoch 3, iter 2600, train loss 1.119, val perplexity 3.10231\n",
      "Epoch 3, iter 2700, train loss 1.083, val perplexity 3.09972\n",
      "Epoch 3, iter 2800, train loss 1.097, val perplexity 3.09806\n",
      "Epoch 3, iter 2900, train loss 1.144, val perplexity 3.09055\n",
      "Epoch 3, iter 3000, train loss 1.093, val perplexity 3.10013\n",
      "Epoch 4, iter 0, train loss 1.088, val perplexity 3.09947\n",
      "Epoch 4, iter 100, train loss 1.138, val perplexity 3.08806\n",
      "Epoch 4, iter 200, train loss 1.073, val perplexity 3.09359\n",
      "Epoch 4, iter 300, train loss 1.120, val perplexity 3.09408\n",
      "Epoch 4, iter 400, train loss 1.191, val perplexity 3.09034\n",
      "Epoch 4, iter 500, train loss 1.083, val perplexity 3.11042\n",
      "Epoch 4, iter 600, train loss 1.118, val perplexity 3.09878\n",
      "Epoch 4, iter 700, train loss 1.131, val perplexity 3.09413\n",
      "Epoch 4, iter 800, train loss 1.103, val perplexity 3.09784\n",
      "Epoch 4, iter 900, train loss 1.164, val perplexity 3.09238\n",
      "Epoch 4, iter 1000, train loss 1.159, val perplexity 3.11123\n",
      "Epoch 4, iter 1100, train loss 1.109, val perplexity 3.09661\n",
      "Epoch 4, iter 1200, train loss 1.141, val perplexity 3.08549\n",
      "Epoch 4, iter 1300, train loss 1.130, val perplexity 3.09496\n",
      "Epoch 4, iter 1400, train loss 1.107, val perplexity 3.08911\n",
      "Epoch 4, iter 1500, train loss 1.161, val perplexity 3.09909\n",
      "Epoch 4, iter 1600, train loss 1.184, val perplexity 3.08785\n",
      "Epoch 4, iter 1700, train loss 1.130, val perplexity 3.10409\n",
      "Epoch 4, iter 1800, train loss 1.141, val perplexity 3.07867\n",
      "Epoch 4, iter 1900, train loss 1.056, val perplexity 3.08581\n",
      "Epoch 4, iter 2000, train loss 1.153, val perplexity 3.07221\n",
      "Epoch 4, iter 2100, train loss 1.092, val perplexity 3.07461\n",
      "Epoch 4, iter 2200, train loss 1.113, val perplexity 3.08047\n",
      "Epoch 4, iter 2300, train loss 1.103, val perplexity 3.08098\n",
      "Epoch 4, iter 2400, train loss 1.146, val perplexity 3.08909\n",
      "Epoch 4, iter 2500, train loss 1.160, val perplexity 3.09073\n",
      "Epoch 4, iter 2600, train loss 1.158, val perplexity 3.08198\n",
      "Epoch 4, iter 2700, train loss 1.117, val perplexity 3.08425\n",
      "Epoch 4, iter 2800, train loss 1.108, val perplexity 3.08411\n",
      "Epoch 4, iter 2900, train loss 1.094, val perplexity 3.07104\n",
      "Epoch 4, iter 3000, train loss 1.141, val perplexity 3.08492\n",
      "Epoch 5, iter 0, train loss 1.118, val perplexity 3.08394\n",
      "Epoch 5, iter 100, train loss 1.144, val perplexity 3.08104\n",
      "Epoch 5, iter 200, train loss 1.095, val perplexity 3.07323\n",
      "Epoch 5, iter 300, train loss 1.150, val perplexity 3.07491\n",
      "Epoch 5, iter 400, train loss 1.151, val perplexity 3.06575\n",
      "Epoch 5, iter 500, train loss 1.128, val perplexity 3.08514\n",
      "Epoch 5, iter 600, train loss 1.082, val perplexity 3.07046\n",
      "Epoch 5, iter 700, train loss 1.111, val perplexity 3.07225\n",
      "Epoch 5, iter 800, train loss 1.125, val perplexity 3.06731\n",
      "Epoch 5, iter 900, train loss 1.173, val perplexity 3.06865\n",
      "Epoch 5, iter 1000, train loss 1.090, val perplexity 3.07417\n",
      "Epoch 5, iter 1100, train loss 1.121, val perplexity 3.08081\n",
      "Epoch 5, iter 1200, train loss 1.189, val perplexity 3.07721\n",
      "Epoch 5, iter 1300, train loss 1.109, val perplexity 3.06918\n",
      "Epoch 5, iter 1400, train loss 1.128, val perplexity 3.06530\n",
      "Epoch 5, iter 1500, train loss 1.103, val perplexity 3.08457\n",
      "Epoch 5, iter 1600, train loss 1.168, val perplexity 3.07169\n",
      "Epoch 5, iter 1700, train loss 1.119, val perplexity 3.06512\n",
      "Epoch 5, iter 1800, train loss 1.117, val perplexity 3.07030\n",
      "Epoch 5, iter 1900, train loss 1.117, val perplexity 3.06689\n",
      "Epoch 5, iter 2000, train loss 1.164, val perplexity 3.07027\n",
      "Epoch 5, iter 2100, train loss 1.126, val perplexity 3.07186\n",
      "Epoch 5, iter 2200, train loss 1.134, val perplexity 3.06953\n",
      "Epoch 5, iter 2300, train loss 1.126, val perplexity 3.07368\n",
      "Epoch 5, iter 2400, train loss 1.145, val perplexity 3.07125\n",
      "Epoch 5, iter 2500, train loss 1.123, val perplexity 3.06302\n",
      "Epoch 5, iter 2600, train loss 1.099, val perplexity 3.06136\n",
      "Epoch 5, iter 2700, train loss 1.151, val perplexity 3.06651\n",
      "Epoch 5, iter 2800, train loss 1.146, val perplexity 3.06840\n",
      "Epoch 5, iter 2900, train loss 1.118, val perplexity 3.06424\n",
      "Epoch 5, iter 3000, train loss 1.145, val perplexity 3.07018\n",
      "Epoch 6, iter 0, train loss 1.083, val perplexity 3.05996\n",
      "Epoch 6, iter 100, train loss 1.141, val perplexity 3.06974\n",
      "Epoch 6, iter 200, train loss 1.123, val perplexity 3.07013\n",
      "Epoch 6, iter 300, train loss 1.087, val perplexity 3.06372\n",
      "Epoch 6, iter 400, train loss 1.154, val perplexity 3.06832\n",
      "Epoch 6, iter 500, train loss 1.152, val perplexity 3.06385\n",
      "Epoch 6, iter 600, train loss 1.149, val perplexity 3.06240\n",
      "Epoch 6, iter 700, train loss 1.122, val perplexity 3.06927\n",
      "Epoch 6, iter 800, train loss 1.083, val perplexity 3.06802\n",
      "Epoch 6, iter 900, train loss 1.139, val perplexity 3.05878\n",
      "Epoch 6, iter 1000, train loss 1.143, val perplexity 3.06103\n",
      "Epoch 6, iter 1100, train loss 1.096, val perplexity 3.06250\n",
      "Epoch 6, iter 1200, train loss 1.155, val perplexity 3.06708\n",
      "Epoch 6, iter 1300, train loss 1.108, val perplexity 3.06496\n",
      "Epoch 6, iter 1400, train loss 1.063, val perplexity 3.06955\n",
      "Epoch 6, iter 1500, train loss 1.114, val perplexity 3.09783\n",
      "Epoch 6, iter 1600, train loss 1.132, val perplexity 3.05893\n",
      "Epoch 6, iter 1700, train loss 1.126, val perplexity 3.06631\n",
      "Epoch 6, iter 1800, train loss 1.114, val perplexity 3.06231\n",
      "Epoch 6, iter 1900, train loss 1.108, val perplexity 3.06608\n",
      "Epoch 6, iter 2000, train loss 1.108, val perplexity 3.06276\n",
      "Epoch 6, iter 2100, train loss 1.113, val perplexity 3.06145\n",
      "Epoch 6, iter 2200, train loss 1.146, val perplexity 3.07341\n",
      "Epoch 6, iter 2300, train loss 1.136, val perplexity 3.06426\n",
      "Epoch 6, iter 2400, train loss 1.123, val perplexity 3.05151\n",
      "Epoch 6, iter 2500, train loss 1.079, val perplexity 3.05730\n",
      "Epoch 6, iter 2600, train loss 1.122, val perplexity 3.05445\n",
      "Epoch 6, iter 2700, train loss 1.154, val perplexity 3.06461\n",
      "Epoch 6, iter 2800, train loss 1.105, val perplexity 3.05673\n",
      "Epoch 6, iter 2900, train loss 1.107, val perplexity 3.05588\n",
      "Epoch 6, iter 3000, train loss 1.155, val perplexity 3.06212\n",
      "Epoch 7, iter 0, train loss 1.090, val perplexity 3.06331\n",
      "Epoch 7, iter 100, train loss 1.107, val perplexity 3.06254\n",
      "Epoch 7, iter 200, train loss 1.139, val perplexity 3.06659\n",
      "Epoch 7, iter 300, train loss 1.086, val perplexity 3.05761\n",
      "Epoch 7, iter 400, train loss 1.116, val perplexity 3.05309\n",
      "Epoch 7, iter 500, train loss 1.108, val perplexity 3.05353\n",
      "Epoch 7, iter 600, train loss 1.151, val perplexity 3.05212\n",
      "Epoch 7, iter 700, train loss 1.099, val perplexity 3.05561\n",
      "Epoch 7, iter 800, train loss 1.116, val perplexity 3.05196\n",
      "Epoch 7, iter 900, train loss 1.117, val perplexity 3.05630\n",
      "Epoch 7, iter 1000, train loss 1.138, val perplexity 3.05843\n",
      "Epoch 7, iter 1100, train loss 1.057, val perplexity 3.06650\n",
      "Epoch 7, iter 1200, train loss 1.144, val perplexity 3.05379\n",
      "Epoch 7, iter 1300, train loss 1.082, val perplexity 3.07219\n",
      "Epoch 7, iter 1400, train loss 1.109, val perplexity 3.05318\n",
      "Epoch 7, iter 1500, train loss 1.128, val perplexity 3.04903\n",
      "Epoch 7, iter 1600, train loss 1.127, val perplexity 3.05185\n",
      "Epoch 7, iter 1700, train loss 1.139, val perplexity 3.05371\n",
      "Epoch 7, iter 1800, train loss 1.151, val perplexity 3.05794\n",
      "Epoch 7, iter 1900, train loss 1.130, val perplexity 3.04564\n",
      "Epoch 7, iter 2000, train loss 1.092, val perplexity 3.06270\n",
      "Epoch 7, iter 2100, train loss 1.144, val perplexity 3.05154\n",
      "Epoch 7, iter 2200, train loss 1.144, val perplexity 3.05436\n",
      "Epoch 7, iter 2300, train loss 1.062, val perplexity 3.04805\n",
      "Epoch 7, iter 2400, train loss 1.094, val perplexity 3.06074\n",
      "Epoch 7, iter 2500, train loss 1.132, val perplexity 3.05435\n",
      "Epoch 7, iter 2600, train loss 1.152, val perplexity 3.04857\n",
      "Epoch 7, iter 2700, train loss 1.172, val perplexity 3.05264\n",
      "Epoch 7, iter 2800, train loss 1.149, val perplexity 3.06710\n",
      "Epoch 7, iter 2900, train loss 1.136, val perplexity 3.04437\n",
      "Epoch 7, iter 3000, train loss 1.107, val perplexity 3.05298\n",
      "Epoch 8, iter 0, train loss 1.124, val perplexity 3.05563\n",
      "Epoch 8, iter 100, train loss 1.100, val perplexity 3.04652\n",
      "Epoch 8, iter 200, train loss 1.120, val perplexity 3.04634\n",
      "Epoch 8, iter 300, train loss 1.122, val perplexity 3.04838\n",
      "Epoch 8, iter 400, train loss 1.122, val perplexity 3.04710\n",
      "Epoch 8, iter 500, train loss 1.135, val perplexity 3.05090\n",
      "Epoch 8, iter 600, train loss 1.147, val perplexity 3.04762\n",
      "Epoch 8, iter 700, train loss 1.158, val perplexity 3.04507\n",
      "Epoch 8, iter 800, train loss 1.113, val perplexity 3.04984\n",
      "Epoch 8, iter 900, train loss 1.110, val perplexity 3.06433\n",
      "Epoch 8, iter 1000, train loss 1.101, val perplexity 3.05136\n",
      "Epoch 8, iter 1100, train loss 1.146, val perplexity 3.05833\n",
      "Epoch 8, iter 1200, train loss 1.177, val perplexity 3.05741\n",
      "Epoch 8, iter 1300, train loss 1.159, val perplexity 3.05038\n",
      "Epoch 8, iter 1400, train loss 1.072, val perplexity 3.04861\n",
      "Epoch 8, iter 1500, train loss 1.088, val perplexity 3.04976\n",
      "Epoch 8, iter 1600, train loss 1.111, val perplexity 3.05190\n",
      "Epoch 8, iter 1700, train loss 1.076, val perplexity 3.04952\n",
      "Epoch 8, iter 1800, train loss 1.082, val perplexity 3.04475\n",
      "Epoch 8, iter 1900, train loss 1.131, val perplexity 3.04838\n",
      "Epoch 8, iter 2000, train loss 1.065, val perplexity 3.05282\n",
      "Epoch 8, iter 2100, train loss 1.137, val perplexity 3.04543\n",
      "Epoch 8, iter 2200, train loss 1.132, val perplexity 3.04434\n",
      "Epoch 8, iter 2300, train loss 1.104, val perplexity 3.05119\n",
      "Epoch 8, iter 2400, train loss 1.134, val perplexity 3.04694\n",
      "Epoch 8, iter 2500, train loss 1.126, val perplexity 3.05060\n",
      "Epoch 8, iter 2600, train loss 1.190, val perplexity 3.05436\n",
      "Epoch 8, iter 2700, train loss 1.118, val perplexity 3.03811\n",
      "Epoch 8, iter 2800, train loss 1.149, val perplexity 3.05414\n",
      "Epoch 8, iter 2900, train loss 1.116, val perplexity 3.04996\n",
      "Epoch 8, iter 3000, train loss 1.139, val perplexity 3.05506\n",
      "Epoch 9, iter 0, train loss 1.152, val perplexity 3.04831\n",
      "Epoch 9, iter 100, train loss 1.120, val perplexity 3.04060\n",
      "Epoch 9, iter 200, train loss 1.102, val perplexity 3.04217\n",
      "Epoch 9, iter 300, train loss 1.110, val perplexity 3.05496\n",
      "Epoch 9, iter 400, train loss 1.105, val perplexity 3.04466\n",
      "Epoch 9, iter 500, train loss 1.153, val perplexity 3.05254\n",
      "Epoch 9, iter 600, train loss 1.156, val perplexity 3.04747\n",
      "Epoch 9, iter 700, train loss 1.119, val perplexity 3.03969\n",
      "Epoch 9, iter 800, train loss 1.138, val perplexity 3.04049\n",
      "Epoch 9, iter 900, train loss 1.066, val perplexity 3.04235\n",
      "Epoch 9, iter 1000, train loss 1.069, val perplexity 3.04608\n",
      "Epoch 9, iter 1100, train loss 1.130, val perplexity 3.05367\n",
      "Epoch 9, iter 1200, train loss 1.069, val perplexity 3.04733\n",
      "Epoch 9, iter 1300, train loss 1.105, val perplexity 3.04638\n",
      "Epoch 9, iter 1400, train loss 1.101, val perplexity 3.04549\n",
      "Epoch 9, iter 1500, train loss 1.167, val perplexity 3.04774\n",
      "Epoch 9, iter 1600, train loss 1.117, val perplexity 3.05246\n",
      "Epoch 9, iter 1700, train loss 1.188, val perplexity 3.05106\n",
      "Epoch 9, iter 1800, train loss 1.084, val perplexity 3.04271\n",
      "Epoch 9, iter 1900, train loss 1.166, val perplexity 3.05037\n",
      "Epoch 9, iter 2000, train loss 1.097, val perplexity 3.04250\n",
      "Epoch 9, iter 2100, train loss 1.109, val perplexity 3.04291\n",
      "Epoch 9, iter 2200, train loss 1.122, val perplexity 3.04075\n",
      "Epoch 9, iter 2300, train loss 1.102, val perplexity 3.04270\n",
      "Epoch 9, iter 2400, train loss 1.138, val perplexity 3.05040\n",
      "Epoch 9, iter 2500, train loss 1.137, val perplexity 3.04009\n",
      "Epoch 9, iter 2600, train loss 1.079, val perplexity 3.04551\n",
      "Epoch 9, iter 2700, train loss 1.133, val perplexity 3.04591\n",
      "Epoch 9, iter 2800, train loss 1.073, val perplexity 3.04346\n",
      "Epoch 9, iter 2900, train loss 1.152, val perplexity 3.04100\n",
      "Epoch 9, iter 3000, train loss 1.084, val perplexity 3.04303\n"
     ]
    }
   ],
   "source": [
    "model = SelfAttentionModel(len(tokenizer.tokens), embedding_dim=128, sequence_length=40, head_size=16)\n",
    "\n",
    "train_model(model, train_loader, valid_loader, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CON#O=#O)O))O)))#=#=#'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[tokenizer.token_to_index('C')]])\n",
    "a = a.to(device)\n",
    "generation = model.generate(a, max_new_tokens=30).cpu().numpy()\n",
    "tokenizer.decode(generation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good model for generating molecules yet ... (even though our validation loss is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Additional perspectives on attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention as GNN\n",
    "\n",
    "- In the attention mechanism we learn how different tokens \"communicate\" with each other. If we think of tokens as nodes, attention corresponds to learning the edge weights of a fully connected graph.\n",
    "\n",
    "- The tokens per default have no notion of their position in the sequence. It is basically the communication between sets of vectors.\n",
    "\n",
    "In attentional GNNs, we write for the embeddings: \n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i=\\phi\\left(\\mathbf{x}_i, \\bigoplus_{j \\in \\mathcal{V}} a\\left(\\mathbf{x}_i, \\mathbf{x}_j\\right) \\psi\\left(\\mathbf{x}_j\\right)\\right)\n",
    "$$ \n",
    "\n",
    "where $\\bigoplus$ is a permutation invariant function, e.g., sum or mean over the neighborhood $\\mathcal{V}$. [Does this equation look familiar?](https://petar-v.com/talks/GNN-EEML.pdf)\n",
    "\n",
    "\n",
    "You can find more information [here](https://thegradient.pub/transformers-are-graph-neural-networks/) and [here](https://arxiv.org/pdf/2301.08210.pdf).\n",
    "\n",
    "The main difference is that in the transformer we model a fully connected graph, whereas in GNNs we model a sparse graph (which is an inductive bias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention as Kernel smoothing\n",
    "\n",
    "- Given that we have been introducing the attention mechanism as a way to compute a weighted average of values, the analogy to a kernel is quite natural.\n",
    "\n",
    "To understand this a bit better, let us introduce [kernel smoothing](https://en.wikipedia.org/wiki/Kernel_regression). Again, it is nothing else then a weighted average. In this weighted average, the weights are determined by a kernel function.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n y_i \\frac{K\\left(x_i, x_o\\right)}{\\sum_{j=1}^n K\\left(x_j, x_o\\right)},\n",
    "$$\n",
    "\n",
    "where $(x_1, y_1), \\dots, (x_n, y_n)$ are the training points and $x_o$ is the point at which we want to make a prediction.\n",
    "\n",
    "A common kernel function is the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "K(x, x_o) = \\exp\\left(xx_o\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a hyperparameter.\n",
    "\n",
    "We are also free to add weights \n",
    "\n",
    "$$\n",
    "K(x, x_o) = \\exp\\left(\\mathbf{w}_1 x  \\mathbf{w}_2 x_o\\right)\n",
    "$$\n",
    "\n",
    "where $w$ are square weight matrices. For stability, we might divide by the dimensionality of $x$.\n",
    "\n",
    "$$\n",
    "K(x, x_o) = \\exp\\left(\\frac{\\mathbf{w}_1 x  \\mathbf{w}_2 x_o}{\\sqrt{d}}\\right)\n",
    "$$\n",
    "\n",
    "where $d$ is the dimensionality of $x$.\n",
    "\n",
    "Compare this to the attention equation:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where $d_k$ is the dimension of $K$ and $Q$.\n",
    "\n",
    "You can find more information on this perspective [here](http://bactra.org/notebooks/nn-attention-and-transformers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more expressive power with more heads and fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple way to improve the attention mechanism is to use multiple attention heads.\n",
    "That is we apply the attention mechanism multiple times and then concatenate the results.\n",
    "\n",
    "The intuition behind this is that different attention heads can learn different attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, n_embed, block_size, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, block_size, head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (B, T, C)\n",
    "        # we want to compute the attention for each head\n",
    "        # and then concatenate the results\n",
    "        # we will have a tensor of shape (B, T, num_heads * head_size)\n",
    "        # in practice, we might not concatenate but add another dimension\n",
    "        # to the tensors\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we let the tokens talk to each other we currently only used one linear layer to map to the outputs. \n",
    "We can expect better performance if we use multiple layers.\n",
    "\n",
    "One typically uses wide linear layers that can more readily be parallelized than deep linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, n_embed, hidden):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_embed)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put it together, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sequence_length=100, head_size=12, num_heads=4):\n",
    "        super().__init__()\n",
    "        # read of the logits of the next token from table\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_embedding = nn.Embedding(sequence_length, embedding_dim)\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.attention = MultiHeadAttention(num_heads, embedding_dim, sequence_length, head_size)\n",
    "        self.feed_forward = FeedForwardLayer(embedding_dim, 4*embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        x += self.positional_embedding(torch.arange(T, device=device))\n",
    "        x = self.attention(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        logits = self.forward(x) # (B, T, C)\n",
    "        B, T, C = logits.shape\n",
    "        # Note that that the implementation below is because of how we - for educational purposes - have defined the dataset\n",
    "        # A better way is to have inputs and outputs of the same length (and to not manually code the sliding window   \n",
    "        logits = logits[:, -1, :]\n",
    "        logits = logits.view(B, C)\n",
    "        y = y.view(B)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens=100):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        # we generate max_new_tokens new tokens\n",
    "        new_tokens = []\n",
    "        for _t in range(max_new_tokens):\n",
    "            x_ = x[:, -self.sequence_length:]\n",
    "            logits = self.forward(x_) # (B, T, C)\n",
    "            logits = logits[:, -1, :] # we only care about the last token in Bigram, hence we bow have shape (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # we generate probabilities for the next token\n",
    "\n",
    "            # torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) \n",
    "            # where each element is the index of the sampled token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            new_tokens.append(next_token)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracting transformers into blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we can improve the performance by performing the self-attention and feedforward multiple times. \n",
    "For this, it is useful to extract the reusable parts into a block.\n",
    "\n",
    "However, just making the model deeper can lead to problems with training. \n",
    "To avoid this, we will leverage two tricks: \n",
    "- we will use residual connections: they allow us to \"skip\" over layers. During optimization, there will be a \"shortcut\" to between the input and the output of the block.\n",
    "- we will use layer normalization: it allows us to normalize the activations of a layer\n",
    "- we will add dropout: it allows us to randomly drop activations during training. This can be seen as a form of regularization.\n",
    "\n",
    "We will apply layer norm twice: \n",
    "- once directly on the input\n",
    "- then before we pass the multihead attention output to the feedforward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that [there is some debate](https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure) on where layer norm is optimally placed.\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? <br><br>It places the layer normalization between the residual blocks, which doesn&#39;t match the code: <a href=\"https://t.co/z1oMLFpmiZ\">https://t.co/z1oMLFpmiZ</a><br><br>PS: This is known as Post-LN Transformer<br><br>1/3 <a href=\"https://t.co/OOvp4FA8Nz\">pic.twitter.com/OOvp4FA8Nz</a></p>&mdash; Sebastian Raschka (@rasbt) <a href=\"https://twitter.com/rasbt/status/1655575611979489282?ref_src=twsrc%5Etfw\">May 8, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, block_size, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(num_heads=n_head, n_embed=n_embd, block_size=block_size, head_size=head_size)\n",
    "        self.ffwd = FeedForwardLayer(n_embd, n_embd*4)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to realize is that a bulk of the parameters is in the `FeedForwardLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sa.heads.0.key.weight': 4096,\n",
       " 'sa.heads.0.query.weight': 4096,\n",
       " 'sa.heads.0.value.weight': 4096,\n",
       " 'sa.heads.1.key.weight': 4096,\n",
       " 'sa.heads.1.query.weight': 4096,\n",
       " 'sa.heads.1.value.weight': 4096,\n",
       " 'sa.heads.2.key.weight': 4096,\n",
       " 'sa.heads.2.query.weight': 4096,\n",
       " 'sa.heads.2.value.weight': 4096,\n",
       " 'sa.heads.3.key.weight': 4096,\n",
       " 'sa.heads.3.query.weight': 4096,\n",
       " 'sa.heads.3.value.weight': 4096,\n",
       " 'ffwd.net.0.weight': 65536,\n",
       " 'ffwd.net.0.bias': 512,\n",
       " 'ffwd.net.2.weight': 65536,\n",
       " 'ffwd.net.2.bias': 128,\n",
       " 'ln1.weight': 128,\n",
       " 'ln1.bias': 128,\n",
       " 'ln2.weight': 128,\n",
       " 'ln2.bias': 128}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = Block(128, 100, 4)\n",
    "get_num_parameters_per_layer(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I fixed the Transformer diagram :D <a href=\"https://t.co/qWnOUjZKut\">pic.twitter.com/qWnOUjZKut</a></p>&mdash; Andrej Karpathy (@karpathy) <a href=\"https://twitter.com/karpathy/status/1658161721251602432?ref_src=twsrc%5Etfw\">May 15, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "![](https://pbs.twimg.com/media/FwL5ROUagAIrsJT?format=jpg&name=medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these \"tricks\" and enhancements of expressivity, we can now build a full GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_blocks):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.layers = nn.Sequential(*[Block(n_embd, block_size, n_head) for _ in range(n_blocks)])\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        \n",
    "        x = self.tok_emb(x) + self.pos_emb(torch.arange(T, device=x.device))  # b,tc, batch, time - seqeuence length, embedding dimension\n",
    "        x = self.layers(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        logits = self.forward(x) # (B, T, C)\n",
    "        B, T, C = logits.shape\n",
    "        # Note that that the implementation below is because of how we - for educational purposes - have defined the dataset\n",
    "        # A better way is to have inputs and outputs of the same length (and to not manually code the sliding window\n",
    "        # but to instead use a causal mask)\n",
    "        logits = logits[:, -1, :]\n",
    "        logits = logits.view(B, C)\n",
    "        y = y.view(B)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def generate(self, x, max_new_tokens=100):\n",
    "        # x is a tensor of shape (B, T)\n",
    "        # we generate max_new_tokens new tokens\n",
    "        new_tokens = []\n",
    "        for _t in range(max_new_tokens):\n",
    "            x_ = x[:, -self.block_size:]\n",
    "            logits = self.forward(x_)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            new_tokens.append(next_token)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(len(tokenizer.tokens), n_embd=64, block_size=40, n_head=4, n_blocks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191360"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parameters(gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not nothing (but still a very small model by today's standards).\n",
    "To increase performance, we can use a larger model, more data, and more training time. For this, we need to use a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, train loss 4.298, val perplexity 33.04309\n",
      "Epoch 0, iter 100, train loss 1.303, val perplexity 3.68513\n",
      "Epoch 0, iter 200, train loss 1.109, val perplexity 3.04452\n",
      "Epoch 0, iter 300, train loss 0.996, val perplexity 2.82078\n",
      "Epoch 0, iter 400, train loss 1.016, val perplexity 2.68037\n",
      "Epoch 0, iter 500, train loss 0.921, val perplexity 2.56847\n",
      "Epoch 0, iter 600, train loss 0.900, val perplexity 2.51342\n",
      "Epoch 0, iter 700, train loss 0.900, val perplexity 2.43911\n",
      "Epoch 0, iter 800, train loss 0.920, val perplexity 2.42055\n",
      "Epoch 0, iter 900, train loss 0.860, val perplexity 2.36890\n",
      "Epoch 0, iter 1000, train loss 0.853, val perplexity 2.36435\n",
      "Epoch 0, iter 1100, train loss 0.839, val perplexity 2.32904\n",
      "Epoch 0, iter 1200, train loss 0.828, val perplexity 2.31901\n",
      "Epoch 0, iter 1300, train loss 0.841, val perplexity 2.27908\n",
      "Epoch 0, iter 1400, train loss 0.824, val perplexity 2.27948\n",
      "Epoch 0, iter 1500, train loss 0.836, val perplexity 2.26692\n",
      "Epoch 0, iter 1600, train loss 0.828, val perplexity 2.24880\n",
      "Epoch 0, iter 1700, train loss 0.784, val perplexity 2.24773\n",
      "Epoch 0, iter 1800, train loss 0.731, val perplexity 2.23166\n",
      "Epoch 0, iter 1900, train loss 0.851, val perplexity 2.23010\n",
      "Epoch 0, iter 2000, train loss 0.813, val perplexity 2.21050\n",
      "Epoch 0, iter 2100, train loss 0.789, val perplexity 2.20347\n",
      "Epoch 0, iter 2200, train loss 0.819, val perplexity 2.20582\n",
      "Epoch 0, iter 2300, train loss 0.821, val perplexity 2.19884\n",
      "Epoch 0, iter 2400, train loss 0.777, val perplexity 2.17299\n",
      "Epoch 0, iter 2500, train loss 0.801, val perplexity 2.17145\n",
      "Epoch 0, iter 2600, train loss 0.778, val perplexity 2.16214\n",
      "Epoch 0, iter 2700, train loss 0.813, val perplexity 2.14991\n",
      "Epoch 0, iter 2800, train loss 0.753, val perplexity 2.14715\n",
      "Epoch 0, iter 2900, train loss 0.782, val perplexity 2.14369\n",
      "Epoch 0, iter 3000, train loss 0.792, val perplexity 2.14781\n",
      "Epoch 1, iter 0, train loss 0.791, val perplexity 2.14369\n",
      "Epoch 1, iter 100, train loss 0.768, val perplexity 2.15356\n",
      "Epoch 1, iter 200, train loss 0.779, val perplexity 2.13835\n",
      "Epoch 1, iter 300, train loss 0.781, val perplexity 2.13887\n",
      "Epoch 1, iter 400, train loss 0.798, val perplexity 2.11990\n",
      "Epoch 1, iter 500, train loss 0.761, val perplexity 2.12426\n",
      "Epoch 1, iter 600, train loss 0.757, val perplexity 2.12802\n",
      "Epoch 1, iter 700, train loss 0.696, val perplexity 2.10705\n",
      "Epoch 1, iter 800, train loss 0.728, val perplexity 2.11449\n",
      "Epoch 1, iter 900, train loss 0.713, val perplexity 2.09770\n",
      "Epoch 1, iter 1000, train loss 0.750, val perplexity 2.10551\n",
      "Epoch 1, iter 1100, train loss 0.760, val perplexity 2.10426\n",
      "Epoch 1, iter 1200, train loss 0.745, val perplexity 2.09038\n",
      "Epoch 1, iter 1300, train loss 0.728, val perplexity 2.09287\n",
      "Epoch 1, iter 1400, train loss 0.724, val perplexity 2.09862\n",
      "Epoch 1, iter 1500, train loss 0.715, val perplexity 2.08002\n",
      "Epoch 1, iter 1600, train loss 0.733, val perplexity 2.07180\n",
      "Epoch 1, iter 1700, train loss 0.759, val perplexity 2.09067\n",
      "Epoch 1, iter 1800, train loss 0.725, val perplexity 2.07620\n",
      "Epoch 1, iter 1900, train loss 0.759, val perplexity 2.07754\n",
      "Epoch 1, iter 2000, train loss 0.751, val perplexity 2.06910\n",
      "Epoch 1, iter 2100, train loss 0.769, val perplexity 2.06469\n",
      "Epoch 1, iter 2200, train loss 0.728, val perplexity 2.06109\n",
      "Epoch 1, iter 2300, train loss 0.718, val perplexity 2.05569\n",
      "Epoch 1, iter 2400, train loss 0.742, val perplexity 2.06637\n",
      "Epoch 1, iter 2500, train loss 0.734, val perplexity 2.05483\n",
      "Epoch 1, iter 2600, train loss 0.726, val perplexity 2.05343\n",
      "Epoch 1, iter 2700, train loss 0.724, val perplexity 2.04880\n",
      "Epoch 1, iter 2800, train loss 0.735, val perplexity 2.05685\n",
      "Epoch 1, iter 2900, train loss 0.678, val perplexity 2.05274\n",
      "Epoch 1, iter 3000, train loss 0.703, val perplexity 2.05303\n",
      "Epoch 2, iter 0, train loss 0.699, val perplexity 2.04484\n",
      "Epoch 2, iter 100, train loss 0.664, val perplexity 2.03955\n",
      "Epoch 2, iter 200, train loss 0.742, val perplexity 2.03363\n",
      "Epoch 2, iter 300, train loss 0.671, val perplexity 2.03902\n",
      "Epoch 2, iter 400, train loss 0.669, val perplexity 2.03420\n",
      "Epoch 2, iter 500, train loss 0.711, val perplexity 2.03897\n",
      "Epoch 2, iter 600, train loss 0.688, val perplexity 2.03722\n",
      "Epoch 2, iter 700, train loss 0.739, val perplexity 2.02792\n",
      "Epoch 2, iter 800, train loss 0.686, val perplexity 2.02749\n",
      "Epoch 2, iter 900, train loss 0.674, val perplexity 2.02256\n",
      "Epoch 2, iter 1000, train loss 0.680, val perplexity 2.01940\n",
      "Epoch 2, iter 1100, train loss 0.706, val perplexity 2.02739\n",
      "Epoch 2, iter 1200, train loss 0.684, val perplexity 2.02100\n",
      "Epoch 2, iter 1300, train loss 0.719, val perplexity 2.02173\n",
      "Epoch 2, iter 1400, train loss 0.719, val perplexity 2.01559\n",
      "Epoch 2, iter 1500, train loss 0.713, val perplexity 2.02232\n",
      "Epoch 2, iter 1600, train loss 0.678, val perplexity 2.02050\n",
      "Epoch 2, iter 1700, train loss 0.699, val perplexity 2.01195\n",
      "Epoch 2, iter 1800, train loss 0.726, val perplexity 2.01648\n",
      "Epoch 2, iter 1900, train loss 0.665, val perplexity 2.00469\n",
      "Epoch 2, iter 2000, train loss 0.731, val perplexity 2.00943\n",
      "Epoch 2, iter 2100, train loss 0.641, val perplexity 2.00431\n",
      "Epoch 2, iter 2200, train loss 0.675, val perplexity 2.00753\n",
      "Epoch 2, iter 2300, train loss 0.701, val perplexity 2.00279\n",
      "Epoch 2, iter 2400, train loss 0.720, val perplexity 2.00466\n",
      "Epoch 2, iter 2500, train loss 0.708, val perplexity 2.00843\n",
      "Epoch 2, iter 2600, train loss 0.701, val perplexity 1.99817\n",
      "Epoch 2, iter 2700, train loss 0.684, val perplexity 1.99712\n",
      "Epoch 2, iter 2800, train loss 0.706, val perplexity 1.99883\n",
      "Epoch 2, iter 2900, train loss 0.645, val perplexity 1.99514\n",
      "Epoch 2, iter 3000, train loss 0.696, val perplexity 1.99474\n",
      "Epoch 3, iter 0, train loss 0.696, val perplexity 1.99485\n",
      "Epoch 3, iter 100, train loss 0.702, val perplexity 1.99606\n",
      "Epoch 3, iter 200, train loss 0.706, val perplexity 1.99615\n",
      "Epoch 3, iter 300, train loss 0.683, val perplexity 1.99672\n",
      "Epoch 3, iter 400, train loss 0.670, val perplexity 1.98804\n",
      "Epoch 3, iter 500, train loss 0.685, val perplexity 1.98392\n",
      "Epoch 3, iter 600, train loss 0.696, val perplexity 1.98886\n",
      "Epoch 3, iter 700, train loss 0.689, val perplexity 1.98799\n",
      "Epoch 3, iter 800, train loss 0.667, val perplexity 1.98932\n",
      "Epoch 3, iter 900, train loss 0.694, val perplexity 1.97782\n",
      "Epoch 3, iter 1000, train loss 0.658, val perplexity 1.98138\n",
      "Epoch 3, iter 1100, train loss 0.663, val perplexity 1.98450\n",
      "Epoch 3, iter 1200, train loss 0.676, val perplexity 1.98185\n",
      "Epoch 3, iter 1300, train loss 0.687, val perplexity 1.98282\n",
      "Epoch 3, iter 1400, train loss 0.669, val perplexity 1.97851\n",
      "Epoch 3, iter 1500, train loss 0.645, val perplexity 1.98293\n",
      "Epoch 3, iter 1600, train loss 0.670, val perplexity 1.97267\n",
      "Epoch 3, iter 1700, train loss 0.655, val perplexity 1.97320\n",
      "Epoch 3, iter 1800, train loss 0.684, val perplexity 1.97549\n",
      "Epoch 3, iter 1900, train loss 0.653, val perplexity 1.97240\n",
      "Epoch 3, iter 2000, train loss 0.683, val perplexity 1.97135\n",
      "Epoch 3, iter 2100, train loss 0.683, val perplexity 1.97329\n",
      "Epoch 3, iter 2200, train loss 0.682, val perplexity 1.97562\n",
      "Epoch 3, iter 2300, train loss 0.656, val perplexity 1.96858\n",
      "Epoch 3, iter 2400, train loss 0.656, val perplexity 1.96842\n",
      "Epoch 3, iter 2500, train loss 0.679, val perplexity 1.97356\n",
      "Epoch 3, iter 2600, train loss 0.690, val perplexity 1.96902\n",
      "Epoch 3, iter 2700, train loss 0.653, val perplexity 1.96474\n",
      "Epoch 3, iter 2800, train loss 0.668, val perplexity 1.96376\n",
      "Epoch 3, iter 2900, train loss 0.698, val perplexity 1.96675\n",
      "Epoch 3, iter 3000, train loss 0.706, val perplexity 1.97032\n",
      "Epoch 4, iter 0, train loss 0.685, val perplexity 1.96571\n",
      "Epoch 4, iter 100, train loss 0.655, val perplexity 1.96267\n",
      "Epoch 4, iter 200, train loss 0.665, val perplexity 1.96190\n",
      "Epoch 4, iter 300, train loss 0.654, val perplexity 1.95672\n",
      "Epoch 4, iter 400, train loss 0.662, val perplexity 1.96184\n",
      "Epoch 4, iter 500, train loss 0.674, val perplexity 1.95858\n",
      "Epoch 4, iter 600, train loss 0.640, val perplexity 1.96036\n",
      "Epoch 4, iter 700, train loss 0.664, val perplexity 1.96075\n",
      "Epoch 4, iter 800, train loss 0.681, val perplexity 1.95997\n",
      "Epoch 4, iter 900, train loss 0.668, val perplexity 1.95221\n",
      "Epoch 4, iter 1000, train loss 0.703, val perplexity 1.95875\n",
      "Epoch 4, iter 1100, train loss 0.661, val perplexity 1.95602\n",
      "Epoch 4, iter 1200, train loss 0.677, val perplexity 1.96031\n",
      "Epoch 4, iter 1300, train loss 0.661, val perplexity 1.95891\n",
      "Epoch 4, iter 1400, train loss 0.661, val perplexity 1.96106\n",
      "Epoch 4, iter 1500, train loss 0.654, val perplexity 1.95425\n",
      "Epoch 4, iter 1600, train loss 0.649, val perplexity 1.95434\n",
      "Epoch 4, iter 1700, train loss 0.678, val perplexity 1.95094\n",
      "Epoch 4, iter 1800, train loss 0.685, val perplexity 1.95337\n",
      "Epoch 4, iter 1900, train loss 0.646, val perplexity 1.94827\n",
      "Epoch 4, iter 2000, train loss 0.667, val perplexity 1.94917\n",
      "Epoch 4, iter 2100, train loss 0.691, val perplexity 1.94871\n",
      "Epoch 4, iter 2200, train loss 0.649, val perplexity 1.94629\n",
      "Epoch 4, iter 2300, train loss 0.664, val perplexity 1.94645\n",
      "Epoch 4, iter 2400, train loss 0.675, val perplexity 1.94734\n",
      "Epoch 4, iter 2500, train loss 0.690, val perplexity 1.94550\n",
      "Epoch 4, iter 2600, train loss 0.661, val perplexity 1.94862\n",
      "Epoch 4, iter 2700, train loss 0.638, val perplexity 1.94786\n",
      "Epoch 4, iter 2800, train loss 0.614, val perplexity 1.95053\n",
      "Epoch 4, iter 2900, train loss 0.650, val perplexity 1.95061\n",
      "Epoch 4, iter 3000, train loss 0.643, val perplexity 1.94821\n",
      "Epoch 5, iter 0, train loss 0.614, val perplexity 1.94505\n",
      "Epoch 5, iter 100, train loss 0.662, val perplexity 1.94390\n",
      "Epoch 5, iter 200, train loss 0.654, val perplexity 1.94004\n",
      "Epoch 5, iter 300, train loss 0.662, val perplexity 1.94811\n",
      "Epoch 5, iter 400, train loss 0.646, val perplexity 1.94678\n",
      "Epoch 5, iter 500, train loss 0.650, val perplexity 1.93871\n",
      "Epoch 5, iter 600, train loss 0.648, val perplexity 1.94281\n",
      "Epoch 5, iter 700, train loss 0.656, val perplexity 1.93877\n",
      "Epoch 5, iter 800, train loss 0.609, val perplexity 1.94154\n",
      "Epoch 5, iter 900, train loss 0.671, val perplexity 1.93844\n",
      "Epoch 5, iter 1000, train loss 0.678, val perplexity 1.93712\n",
      "Epoch 5, iter 1100, train loss 0.660, val perplexity 1.93813\n",
      "Epoch 5, iter 1200, train loss 0.665, val perplexity 1.93745\n",
      "Epoch 5, iter 1300, train loss 0.645, val perplexity 1.93484\n",
      "Epoch 5, iter 1400, train loss 0.709, val perplexity 1.94073\n",
      "Epoch 5, iter 1500, train loss 0.665, val perplexity 1.93776\n",
      "Epoch 5, iter 1600, train loss 0.652, val perplexity 1.93378\n",
      "Epoch 5, iter 1700, train loss 0.650, val perplexity 1.93217\n",
      "Epoch 5, iter 1800, train loss 0.652, val perplexity 1.93201\n",
      "Epoch 5, iter 1900, train loss 0.656, val perplexity 1.93616\n",
      "Epoch 5, iter 2000, train loss 0.634, val perplexity 1.93369\n",
      "Epoch 5, iter 2100, train loss 0.682, val perplexity 1.92987\n",
      "Epoch 5, iter 2200, train loss 0.658, val perplexity 1.93708\n",
      "Epoch 5, iter 2300, train loss 0.684, val perplexity 1.93574\n",
      "Epoch 5, iter 2400, train loss 0.647, val perplexity 1.92982\n",
      "Epoch 5, iter 2500, train loss 0.685, val perplexity 1.93247\n",
      "Epoch 5, iter 2600, train loss 0.659, val perplexity 1.93094\n",
      "Epoch 5, iter 2700, train loss 0.638, val perplexity 1.92487\n",
      "Epoch 5, iter 2800, train loss 0.647, val perplexity 1.93157\n",
      "Epoch 5, iter 2900, train loss 0.654, val perplexity 1.93201\n",
      "Epoch 5, iter 3000, train loss 0.612, val perplexity 1.93179\n",
      "Epoch 6, iter 0, train loss 0.642, val perplexity 1.92702\n",
      "Epoch 6, iter 100, train loss 0.600, val perplexity 1.92657\n",
      "Epoch 6, iter 200, train loss 0.656, val perplexity 1.92852\n",
      "Epoch 6, iter 300, train loss 0.661, val perplexity 1.92760\n",
      "Epoch 6, iter 400, train loss 0.619, val perplexity 1.92550\n",
      "Epoch 6, iter 500, train loss 0.664, val perplexity 1.92921\n",
      "Epoch 6, iter 600, train loss 0.628, val perplexity 1.92517\n",
      "Epoch 6, iter 700, train loss 0.659, val perplexity 1.92601\n",
      "Epoch 6, iter 800, train loss 0.657, val perplexity 1.92403\n",
      "Epoch 6, iter 900, train loss 0.654, val perplexity 1.92444\n",
      "Epoch 6, iter 1000, train loss 0.632, val perplexity 1.92993\n",
      "Epoch 6, iter 1100, train loss 0.638, val perplexity 1.92607\n",
      "Epoch 6, iter 1200, train loss 0.619, val perplexity 1.93088\n",
      "Epoch 6, iter 1300, train loss 0.662, val perplexity 1.92206\n",
      "Epoch 6, iter 1400, train loss 0.630, val perplexity 1.92396\n",
      "Epoch 6, iter 1500, train loss 0.651, val perplexity 1.92477\n",
      "Epoch 6, iter 1600, train loss 0.630, val perplexity 1.92285\n",
      "Epoch 6, iter 1700, train loss 0.648, val perplexity 1.92193\n",
      "Epoch 6, iter 1800, train loss 0.638, val perplexity 1.92576\n",
      "Epoch 6, iter 1900, train loss 0.639, val perplexity 1.92451\n",
      "Epoch 6, iter 2000, train loss 0.640, val perplexity 1.92449\n",
      "Epoch 6, iter 2100, train loss 0.668, val perplexity 1.91900\n",
      "Epoch 6, iter 2200, train loss 0.644, val perplexity 1.92026\n",
      "Epoch 6, iter 2300, train loss 0.643, val perplexity 1.92168\n",
      "Epoch 6, iter 2400, train loss 0.614, val perplexity 1.92009\n",
      "Epoch 6, iter 2500, train loss 0.658, val perplexity 1.91711\n",
      "Epoch 6, iter 2600, train loss 0.632, val perplexity 1.92131\n",
      "Epoch 6, iter 2700, train loss 0.622, val perplexity 1.91643\n",
      "Epoch 6, iter 2800, train loss 0.647, val perplexity 1.92231\n",
      "Epoch 6, iter 2900, train loss 0.643, val perplexity 1.91307\n",
      "Epoch 6, iter 3000, train loss 0.682, val perplexity 1.91227\n",
      "Epoch 7, iter 0, train loss 0.640, val perplexity 1.90957\n",
      "Epoch 7, iter 100, train loss 0.664, val perplexity 1.91564\n",
      "Epoch 7, iter 200, train loss 0.623, val perplexity 1.91433\n",
      "Epoch 7, iter 300, train loss 0.624, val perplexity 1.92183\n",
      "Epoch 7, iter 400, train loss 0.636, val perplexity 1.91515\n",
      "Epoch 7, iter 500, train loss 0.646, val perplexity 1.91232\n",
      "Epoch 7, iter 600, train loss 0.666, val perplexity 1.91783\n",
      "Epoch 7, iter 700, train loss 0.627, val perplexity 1.91541\n",
      "Epoch 7, iter 800, train loss 0.650, val perplexity 1.91628\n",
      "Epoch 7, iter 900, train loss 0.644, val perplexity 1.91716\n",
      "Epoch 7, iter 1000, train loss 0.637, val perplexity 1.91660\n",
      "Epoch 7, iter 1100, train loss 0.650, val perplexity 1.91331\n",
      "Epoch 7, iter 1200, train loss 0.624, val perplexity 1.90559\n",
      "Epoch 7, iter 1300, train loss 0.658, val perplexity 1.92101\n",
      "Epoch 7, iter 1400, train loss 0.634, val perplexity 1.90895\n",
      "Epoch 7, iter 1500, train loss 0.656, val perplexity 1.91256\n",
      "Epoch 7, iter 1600, train loss 0.607, val perplexity 1.91214\n",
      "Epoch 7, iter 1700, train loss 0.673, val perplexity 1.91355\n",
      "Epoch 7, iter 1800, train loss 0.664, val perplexity 1.91474\n",
      "Epoch 7, iter 1900, train loss 0.620, val perplexity 1.91403\n",
      "Epoch 7, iter 2000, train loss 0.632, val perplexity 1.90918\n",
      "Epoch 7, iter 2100, train loss 0.617, val perplexity 1.90886\n",
      "Epoch 7, iter 2200, train loss 0.626, val perplexity 1.91003\n",
      "Epoch 7, iter 2300, train loss 0.626, val perplexity 1.91492\n",
      "Epoch 7, iter 2400, train loss 0.670, val perplexity 1.91278\n",
      "Epoch 7, iter 2500, train loss 0.641, val perplexity 1.90954\n",
      "Epoch 7, iter 2600, train loss 0.641, val perplexity 1.90833\n",
      "Epoch 7, iter 2700, train loss 0.599, val perplexity 1.91203\n",
      "Epoch 7, iter 2800, train loss 0.623, val perplexity 1.90824\n",
      "Epoch 7, iter 2900, train loss 0.635, val perplexity 1.90809\n",
      "Epoch 7, iter 3000, train loss 0.611, val perplexity 1.90536\n",
      "Epoch 8, iter 0, train loss 0.659, val perplexity 1.90385\n",
      "Epoch 8, iter 100, train loss 0.588, val perplexity 1.90728\n",
      "Epoch 8, iter 200, train loss 0.651, val perplexity 1.90850\n",
      "Epoch 8, iter 300, train loss 0.622, val perplexity 1.90681\n",
      "Epoch 8, iter 400, train loss 0.653, val perplexity 1.90605\n",
      "Epoch 8, iter 500, train loss 0.625, val perplexity 1.91272\n",
      "Epoch 8, iter 600, train loss 0.663, val perplexity 1.90805\n",
      "Epoch 8, iter 700, train loss 0.622, val perplexity 1.90687\n",
      "Epoch 8, iter 800, train loss 0.641, val perplexity 1.91108\n",
      "Epoch 8, iter 900, train loss 0.657, val perplexity 1.91231\n",
      "Epoch 8, iter 1000, train loss 0.653, val perplexity 1.90258\n",
      "Epoch 8, iter 1100, train loss 0.583, val perplexity 1.91110\n",
      "Epoch 8, iter 1200, train loss 0.630, val perplexity 1.90232\n",
      "Epoch 8, iter 1300, train loss 0.667, val perplexity 1.90653\n",
      "Epoch 8, iter 1400, train loss 0.623, val perplexity 1.90990\n",
      "Epoch 8, iter 1500, train loss 0.618, val perplexity 1.90475\n",
      "Epoch 8, iter 1600, train loss 0.634, val perplexity 1.91038\n",
      "Epoch 8, iter 1700, train loss 0.619, val perplexity 1.90506\n",
      "Epoch 8, iter 1800, train loss 0.615, val perplexity 1.90316\n",
      "Epoch 8, iter 1900, train loss 0.676, val perplexity 1.90349\n",
      "Epoch 8, iter 2000, train loss 0.635, val perplexity 1.90059\n",
      "Epoch 8, iter 2100, train loss 0.668, val perplexity 1.90064\n",
      "Epoch 8, iter 2200, train loss 0.665, val perplexity 1.89685\n",
      "Epoch 8, iter 2300, train loss 0.641, val perplexity 1.90343\n",
      "Epoch 8, iter 2400, train loss 0.630, val perplexity 1.90410\n",
      "Epoch 8, iter 2500, train loss 0.622, val perplexity 1.90396\n",
      "Epoch 8, iter 2600, train loss 0.614, val perplexity 1.90168\n",
      "Epoch 8, iter 2700, train loss 0.638, val perplexity 1.90356\n",
      "Epoch 8, iter 2800, train loss 0.629, val perplexity 1.89678\n",
      "Epoch 8, iter 2900, train loss 0.631, val perplexity 1.90075\n",
      "Epoch 8, iter 3000, train loss 0.683, val perplexity 1.90022\n",
      "Epoch 9, iter 0, train loss 0.647, val perplexity 1.89580\n",
      "Epoch 9, iter 100, train loss 0.620, val perplexity 1.89712\n",
      "Epoch 9, iter 200, train loss 0.661, val perplexity 1.89832\n",
      "Epoch 9, iter 300, train loss 0.613, val perplexity 1.90271\n",
      "Epoch 9, iter 400, train loss 0.653, val perplexity 1.89809\n",
      "Epoch 9, iter 500, train loss 0.629, val perplexity 1.89769\n",
      "Epoch 9, iter 600, train loss 0.633, val perplexity 1.89504\n",
      "Epoch 9, iter 700, train loss 0.652, val perplexity 1.89997\n",
      "Epoch 9, iter 800, train loss 0.636, val perplexity 1.89953\n",
      "Epoch 9, iter 900, train loss 0.640, val perplexity 1.89767\n",
      "Epoch 9, iter 1000, train loss 0.632, val perplexity 1.89748\n",
      "Epoch 9, iter 1100, train loss 0.615, val perplexity 1.89508\n",
      "Epoch 9, iter 1200, train loss 0.663, val perplexity 1.90394\n",
      "Epoch 9, iter 1300, train loss 0.636, val perplexity 1.89673\n",
      "Epoch 9, iter 1400, train loss 0.636, val perplexity 1.89609\n",
      "Epoch 9, iter 1500, train loss 0.600, val perplexity 1.89495\n",
      "Epoch 9, iter 1600, train loss 0.608, val perplexity 1.90642\n",
      "Epoch 9, iter 1700, train loss 0.614, val perplexity 1.89560\n",
      "Epoch 9, iter 1800, train loss 0.631, val perplexity 1.89916\n",
      "Epoch 9, iter 1900, train loss 0.640, val perplexity 1.89817\n",
      "Epoch 9, iter 2000, train loss 0.617, val perplexity 1.89502\n",
      "Epoch 9, iter 2100, train loss 0.659, val perplexity 1.89615\n",
      "Epoch 9, iter 2200, train loss 0.628, val perplexity 1.89633\n",
      "Epoch 9, iter 2300, train loss 0.624, val perplexity 1.89318\n",
      "Epoch 9, iter 2400, train loss 0.613, val perplexity 1.89063\n",
      "Epoch 9, iter 2500, train loss 0.644, val perplexity 1.89450\n",
      "Epoch 9, iter 2600, train loss 0.619, val perplexity 1.89485\n",
      "Epoch 9, iter 2700, train loss 0.649, val perplexity 1.89480\n",
      "Epoch 9, iter 2800, train loss 0.675, val perplexity 1.89209\n",
      "Epoch 9, iter 2900, train loss 0.665, val perplexity 1.89682\n",
      "Epoch 9, iter 3000, train loss 0.633, val perplexity 1.89675\n"
     ]
    }
   ],
   "source": [
    "train_model(gpt, train_loader, valid_loader, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "torch.save(gpt.state_dict(), 'gpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "\n",
    "for i in range(500):\n",
    "    a = torch.tensor([[tokenizer.token_to_index('C')]])\n",
    "    a = a.to(device)\n",
    "    generation = gpt.generate(a, max_new_tokens=30).cpu().numpy()\n",
    "    generations.append(tokenizer.decode(generation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C1CCC1', 'C=COCC1CCNC(C', 'C1C1CC1CC1CCCCCCCC1CC1', 'C', 'C1CO',\n",
       "       'C1C1CC1CCCC1CCC1CCCCC[C@@H]1', 'C1C', 'C(OC(F)c1', 'C=CC',\n",
       "       'C1C1C1', 'C1CC1C1', 'C', 'CC1C1C1', 'C1C1CC1C1CCC1c1CCCCCC1',\n",
       "       'C1C1CC1C1CCCCCCC1C1', 'CC1(C)C1C', 'C(=C', 'C=CCNc1Oc1C(CCCCC',\n",
       "       'C1C1CCCCCCC1', 'C1CC1C1', 'C1CC1C1CCCCCCC1C1CCCCCC1',\n",
       "       'C1[C@H](=CNCC(Cc', 'C', 'C', 'C1CCC', 'C', 'C1C1C2CCCCCC1',\n",
       "       'C1C1CC1C1CCCCCCCCC1', 'C', 'C1C1CCC1CC1CCCCCCCCCC1', 'C',\n",
       "       'C=COC1', 'C1C1CC1', 'C1CC1C1', 'C1', 'C1C1C1', 'C[O-]1C1CCCCC1',\n",
       "       'C1C1CC1', 'CC1(C)C1CCC1',\n",
       "       'C1C1CC1C[C@@H]1CC1C1CCCC[C@@H]1CCCCCCC1', 'C', 'C1C1CC1C1CCC1',\n",
       "       'C1CCC1', 'C1CC1C1C', 'CCC[C@@H](C(=CC', 'C1CC1', 'C=CN', 'C=C',\n",
       "       'C1CC1C1C1CCC', 'C1CC1C1CCCCCC1', 'C1CCC1',\n",
       "       'C[O-]1C1CCCC1CCCC1CCC1C1CCC1', 'C(', 'C(OC(F)c1', 'C1C1CC1', 'C=',\n",
       "       'C', 'C=CN', 'C1CCC1', 'CC1(C)C1C1', 'C', 'C1C1CCCCCCC1', 'C1',\n",
       "       'C1CCC1', 'C=', 'C1C1CC1CCC1', 'C1', 'C1C1CC1CCCCCCC1CC1',\n",
       "       'CC1C1C1', 'C1C1CC1', 'C', 'C1C1CCC1CCC1CCCCCCC1', 'C1C1C2',\n",
       "       'C1CCCC1', 'C(C(', 'C=COCCc1Cc1F', 'CFCc1NC(=C(=CF)', 'C',\n",
       "       'C1C1CC1CCCCCCC1CC1', 'C1C1CC1CCC[C@@H]1C1C1C1CCCCC1', 'C=c1',\n",
       "       'CCO[C@H]1NCn2CC', 'C1CC1C1', 'C1C1CC1', 'C[O-]', 'CC1C1C1',\n",
       "       'C=C[C@H]1', 'C1CC1C1CCCCCC[C@@H]1C[C@@H]1CC1CCC1', 'C',\n",
       "       'C1C1CC1C1CCC1CCCCCCC1', 'C1C1CC1C1CCC1c1CCCCCC1', 'C=C1CN[C@@H]',\n",
       "       'C1CCC1', 'C1CC1C1', 'C1C1CCC1CCCC1CCCC1CCC1', 'C=CN', 'C',\n",
       "       'C=CCCNc1Cccc1Cn1C', 'CC1C1C1', 'C1CC1C1'], dtype='<U41')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(generations, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerations.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgenerations\u001b[49m:\n\u001b[1;32m      3\u001b[0m         handle\u001b[38;5;241m.\u001b[39mwrite(generation \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation\u001b[39m(data):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#the small validitation part\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generations' is not defined"
     ]
    }
   ],
   "source": [
    "with open('generations.txt', 'w') as handle:\n",
    "    for generation in generations:\n",
    "        handle.write(generation + '\\n')\n",
    "\n",
    "\n",
    "def validation(data):\n",
    "#the small validitation part\n",
    "    valid_data = []\n",
    "    invalid_data = []\n",
    "    \n",
    "    for i in data:\n",
    "        if validate_smiles(i):\n",
    "            valid_data.append(i)\n",
    "        else:\n",
    "            invalid_data.append(i)\n",
    "    \n",
    "    print(\"this is the valid data\",len(valid_data))\n",
    "    \n",
    "    print(\"this is the invalid data\",len(invalid_data))\n",
    "    \n",
    "    print(\"len whole data\",len(whole_data))\n",
    "    \n",
    "    print(\"valid percent %\",len(valid_data) * 100 / len(whole_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those seem the best we have seen so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how to build a GPT to generate new SMILES.\n",
    "We generalized a simple bigram model to take into account all past tokens and not just the last one.\n",
    "When we take the tokens into account, we do this by using self-attention, which allows the model to learn the dependencies between tokens.\n",
    "\n",
    "To further improve the model, we added multiple heads to the self-attention mechanism, which allows the model to learn different dependencies between tokens.\n",
    "Finally, we stacked multiple blocks of self-attention and feedforward layers to create a GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "Much of this discussion (and also the way it is structured, e.g., based on the bigram) is based on the outstanding material created by [Andrej Karpathy](https://karpathy.ai/zero-to-hero.html). In particular, the implementation here follows [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
    "\n",
    "Other useful resources are:\n",
    "\n",
    "- [Annotated transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [Illustrated transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Attention! Attention?](https://lilianweng.github.io/posts/2018-06-24-attention/)\n",
    "- [Interactive attention visualization](https://bbycroft.net/llm)\n",
    "- [Simon Prince's book](https://udlbook.github.io/udlbook/) and [blog posts](https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/) have very nice illustrations of the attention mechanism.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
